{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.autocast import TorchAutocast\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "folder = './musicgen-stereo-small'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "prompt = '80s pop track with bassy drums and synth'\n",
    "\n",
    "cfg = 5\n",
    "temperature = 0.7\n",
    "top_k = 500\n",
    "top_p = 0.0\n",
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "model = MusicGen.get_pretrained(\"facebook/musicgen-stereo-small\", device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractions of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ConditioningAttributes(text={'description': '80s pop track with bassy drums and synth'}, wav={'self_wav': WavCondition(wav=tensor([[[0.]]]), length=tensor([0]), sample_rate=[32000], path=[None], seek_time=[])}, joint_embed={})]\n",
      "torch.Size([2, 12, 768])\n",
      "torch.Size([1024, 768])\n",
      "yeet\n",
      "tensor([[[-0.2954,  0.0587, -0.0238,  ...,  0.0086, -0.0069, -0.0077]],\n",
      "\n",
      "        [[-0.2954,  0.0587, -0.0238,  ...,  0.0086, -0.0069, -0.0077]]])\n",
      "{'description': (tensor([[[ 0.3144,  0.1624,  0.1167,  ...,  0.4033,  0.3492,  0.5680],\n",
      "         [-0.0924,  0.2206,  0.6471,  ..., -0.2670,  0.6927,  1.0012],\n",
      "         [-0.2491, -0.4061,  0.0845,  ..., -0.2199, -0.7242,  0.6362],\n",
      "         ...,\n",
      "         [ 0.1737,  0.3411, -0.2478,  ...,  0.1705,  0.1207,  0.0260],\n",
      "         [ 0.4187,  0.2239,  0.1036,  ...,  0.1287,  0.2915, -0.6879],\n",
      "         [-0.0094, -0.0076, -0.0243,  ...,  0.0028, -0.0176, -0.0108]],\n",
      "\n",
      "        [[-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000]]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))}\n",
      "LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "_streaming_state {}\n",
      "tensor([[[1491],\n",
      "         [ 673],\n",
      "         [  31],\n",
      "         [ 807],\n",
      "         [1232],\n",
      "         [1007],\n",
      "         [1506],\n",
      "         [1441]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/audiocraft/models/genmodel.py:162\u001b[0m, in \u001b[0;36mBaseGenModel.generate\u001b[0;34m(self, descriptions, progress, return_tokens)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(attributes)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m prompt_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/audiocraft/models/musicgen.py:256\u001b[0m, in \u001b[0;36mMusicGen._generate_tokens\u001b[0;34m(self, attributes, prompt_tokens, progress)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_duration:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# generate by sampling from LM, simple case.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast:\n\u001b[0;32m--> 256\u001b[0m         gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_gen_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# now this gets a bit messier, we need to handle prompts,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# melody conditioning etc.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     ref_wavs \u001b[38;5;241m=\u001b[39m [attr\u001b[38;5;241m.\u001b[39mwav[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_wav\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes]\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/audiocraft/models/lm.py:519\u001b[0m, in \u001b[0;36mLMModel.generate\u001b[0;34m(self, prompt, conditions, num_samples, max_gen_len, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg, remove_prompts, check, callback, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_next_token(\n\u001b[1;32m    516\u001b[0m     curr_sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p,\n\u001b[1;32m    517\u001b[0m     cfg_coef\u001b[38;5;241m=\u001b[39mcfg_coef, two_step_cfg\u001b[38;5;241m=\u001b[39mtwo_step_cfg)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_token)\n\u001b[0;32m--> 519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# ensure the tokens that should be masked are properly set to special_token_id\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# as the model never output special_token_id\u001b[39;00m\n\u001b[1;32m    522\u001b[0m valid_mask \u001b[38;5;241m=\u001b[39m mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, offset:offset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "wav = model.generate([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "sampling_rate = 32000\n",
    "Audio(wav[0].detach().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfolding the Streaming Transformer to just the regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreamingTransformer(\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x StreamingTransformerLayer(\n",
       "      (self_attn): StreamingMultiheadAttention(\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (layer_scale_1): Identity()\n",
       "      (layer_scale_2): Identity()\n",
       "      (cross_attention): StreamingMultiheadAttention(\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (dropout_cross): Dropout(p=0.0, inplace=False)\n",
       "      (norm_cross): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_scale_cross): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.modules.transformer import StreamingTransformerLayer, create_sin_embedding\n",
    "\n",
    "class UnwrappedTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_layers = 24\n",
    "\n",
    "        self.positional_embedding = 'sin'\n",
    "        self.max_period = 10000\n",
    "        self.positional_scale = 1.0\n",
    "        self.weight_decay = None\n",
    "        self.lr = None\n",
    "        self.rope = None\n",
    "        self.checkpointing = 'none'\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(num_layers):\n",
    "            self.layers.append(\n",
    "                StreamingTransformerLayer(\n",
    "                    d_model=1024, num_heads=16, dim_feedforward=4096,\n",
    "                    dropout=0.0, bias_ff=False, bias_attn=False,\n",
    "                    causal=True, past_context=None, custom=False,\n",
    "                    memory_efficient=True, attention_as_float32=False,\n",
    "                    cross_attention=True, layer_scale=None, rope=self.rope,\n",
    "                    device='cpu', dtype=torch.float32, **{'norm': 'layer_norm', 'norm_first': True, 'activation': 'gelu', 'qk_layer_norm': False, 'qk_layer_norm_cross': False, 'attention_dropout': None, 'kv_repeat': 1})\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, offsets, *args, **kwargs):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        if self.positional_embedding in ['sin', 'sin_rope']:\n",
    "            positions = torch.arange(T, device=x.device).view(1, -1, 1)\n",
    "            positions = positions + offsets.view(-1, 1, 1)\n",
    "            pos_emb = create_sin_embedding(positions, C, max_period=self.max_period, dtype=x.dtype)\n",
    "            x = x + self.positional_scale * pos_emb\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "\n",
    "        offsets = offsets + T\n",
    "\n",
    "        return x, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1435, 0.8727, 0.0470,  ..., 1.8955, 0.3980, 0.5159]],\n",
      "\n",
      "        [[1.2410, 0.8026, 0.1755,  ..., 1.9652, 0.2593, 0.6302]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2954,  0.0587, -0.0238,  ...,  0.0086, -0.0069, -0.0077]],\n",
       " \n",
       "         [[-0.2954,  0.0587, -0.0238,  ...,  0.0086, -0.0069, -0.0077]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.3144,  0.1624,  0.1167,  ...,  0.4033,  0.3492,  0.5680],\n",
       "          [-0.0924,  0.2206,  0.6471,  ..., -0.2670,  0.6927,  1.0012],\n",
       "          [-0.2491, -0.4061,  0.0845,  ..., -0.2199, -0.7242,  0.6362],\n",
       "          ...,\n",
       "          [ 0.1737,  0.3411, -0.2478,  ...,  0.1705,  0.1207,  0.0260],\n",
       "          [ 0.4187,  0.2239,  0.1036,  ...,  0.1287,  0.2915, -0.6879],\n",
       "          [-0.0094, -0.0076, -0.0243,  ...,  0.0028, -0.0176, -0.0108]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,    11,\n",
       "          13353,     1],\n",
       "         [    1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from audiocraft.modules.codebooks_patterns import Pattern\n",
    "\n",
    "projection_layer = model.lm.condition_provider.conditioners.description.output_proj # Take the embed enlargement layer\n",
    "embbedding_layer = model.lm.emb\n",
    "fuser = model.lm.fuser\n",
    "transformer = UnwrappedTransformer()\n",
    "transformer.load_state_dict(model.lm.transformer.state_dict())\n",
    "\n",
    "class LMModelWrapper(nn.Module):\n",
    "    def __init__(self, projection_layer, emb, fuser, transformer): #, lm_model, projection_layer):\n",
    "        super().__init__()\n",
    "        # Layers\n",
    "        self.projection = projection_layer\n",
    "        self.emb = emb\n",
    "        self.fuser = fuser\n",
    "        self.transformer = transformer\n",
    "\n",
    "        #Vars\n",
    "        self.num_codebooks = 8\n",
    "        self.audio_channels = 2\n",
    "\n",
    "    def triu_onnx(x, diagonal=0):\n",
    "        l, w = x.shape\n",
    "        arange_rows = torch.arange(l, device=x.device)\n",
    "\n",
    "        arange_cols = torch.arange(w, device=x.device)\n",
    "        mask = arange_cols.expand(l, w)\n",
    "\n",
    "        arange_rows = arange_rows[:, None] + diagonal\n",
    "        mask = mask >= arange_rows\n",
    "        return x.masked_fill(mask == 0, 0)\n",
    "\n",
    "    def build_delay_pattern_mask(self, input_ids, pad_token_id, max_length):\n",
    "        input_ids = input_ids.reshape(-1, self.num_codebooks, input_ids.shape[-1])\n",
    "        bsz, num_codebooks, seq_len = input_ids.shape\n",
    "\n",
    "        max_length = max_length.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "        condition = (max_length > 0).long()  # Create a condition tensor\n",
    "\n",
    "        input_ids_shifted = condition * (\n",
    "            torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
    "        )\n",
    "\n",
    "        max_length = max_length.item()\n",
    "\n",
    "        channel_codebooks = num_codebooks // 2 if self.audio_channels == 2 else num_codebooks\n",
    "        # we only apply the mask if we have a large enough seq len - otherwise we return as is\n",
    "        if max_length < 2 * channel_codebooks - 1:\n",
    "            return input_ids.reshape(bsz * num_codebooks, -1), input_ids_shifted.reshape(bsz * num_codebooks, -1)\n",
    "\n",
    "        # fill the shifted ids with the prompt entries, offset by the codebook idx\n",
    "        for codebook in range(channel_codebooks):\n",
    "            if self.audio_channels == 1:\n",
    "                # mono channel - loop over the codebooks one-by-one\n",
    "                input_ids_shifted[:, codebook, codebook : seq_len + codebook] = input_ids[:, codebook]\n",
    "            else:\n",
    "                # left/right channels are interleaved in the generated codebooks, so handle one then the other\n",
    "                input_ids_shifted[:, 2 * codebook, codebook : seq_len + codebook] = input_ids[:, 2 * codebook]\n",
    "                input_ids_shifted[:, 2 * codebook + 1, codebook : seq_len + codebook] = input_ids[:, 2 * codebook + 1]\n",
    "        # construct a pattern mask that indicates the positions of padding tokens for each codebook\n",
    "        # first fill the upper triangular part (the EOS padding)\n",
    "        delay_pattern = torch.triu(\n",
    "            torch.ones((channel_codebooks, max_length), dtype=torch.bool), diagonal=max_length - channel_codebooks + 1\n",
    "        ).to(torch.int64)\n",
    "        # then fill the lower triangular part (the BOS padding)\n",
    "        delay_pattern = delay_pattern + torch.tril(torch.ones((channel_codebooks, max_length), dtype=torch.int64))\n",
    "\n",
    "        if self.audio_channels == 2:\n",
    "            # for left/right channel we need to duplicate every row of the pattern mask in an interleaved fashion\n",
    "            delay_pattern = delay_pattern.repeat_interleave(2, dim=0)\n",
    "\n",
    "        delay_pattern = delay_pattern.to(torch.bool)\n",
    "\n",
    "        mask = ~delay_pattern.to(input_ids.device)\n",
    "        input_ids = mask * input_ids_shifted + ~mask * pad_token_id\n",
    "\n",
    "        # find the first position to start generating - this is the first place we have the -1 token\n",
    "        # and will always be in the first codebook (since it has no codebook offset)\n",
    "        first_codebook_ids = input_ids[:, 0, :]\n",
    "        start_ids = (first_codebook_ids == -1).nonzero()[:, 1]\n",
    "        if len(start_ids) > 0:\n",
    "            first_start_id = min(start_ids)\n",
    "        else:\n",
    "            # we have no tokens that need to be filled - return entire matrix of input ids\n",
    "            first_start_id = seq_len\n",
    "\n",
    "        # (bsz * num_codebooks, seq_len) -> (bsz, num_codebooks, seq_len)\n",
    "        pattern_mask = input_ids.reshape(bsz * num_codebooks, -1)\n",
    "        input_ids = input_ids[..., :first_start_id].reshape(bsz * num_codebooks, -1)\n",
    "        \n",
    "        return input_ids, pattern_mask \n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            embeddings: torch.Tensor,           # From the T5 Model\n",
    "            attention_mask: torch.Tensor,       # From the Tokenizer\n",
    "            tokens: torch.Tensor,               # n_samples, codebooks, tokens\n",
    "        ):\n",
    "        cfg = torch.tensor([5])\n",
    "        max_length = torch.tensor([256])\n",
    "        embeddings = self.projection(embeddings.to(self.projection.weight))\n",
    "        embeddings = (embeddings * attention_mask.unsqueeze(-1))\n",
    "\n",
    "        num_samples, num_codebooks = tokens.size(0)-1, 8 # Find a better way to extract this\n",
    "\n",
    "        input_ids = torch.tensor([[2048]] * self.num_codebooks)\n",
    "        input_ids, pattern = self.build_delay_pattern_mask(input_ids=input_ids, pad_token_id=2048, max_length=max_length)\n",
    "        mask = torch.where(pattern == -1, 1, 0)\n",
    "\n",
    "        prev_offset = 0\n",
    "        offset_transformer = torch.zeros(tokens.size(0), dtype=torch.long)\n",
    "\n",
    "        for offset in range(1, max_length):\n",
    "            curr_sequence = pattern[..., prev_offset : offset].unsqueeze(0)\n",
    "            curr_mask = mask[None, ..., prev_offset:offset].expand(num_samples, -1, -1)\n",
    "\n",
    "            # Similar to the _sample_next_token fn\n",
    "            sequence = torch.cat([curr_sequence, curr_sequence], dim=0)\n",
    "\n",
    "\n",
    "            # Go into the forward of the LM model\n",
    "            B, K, S = sequence.shape\n",
    "            input_ = sum([self.emb[k](sequence[:, k]) for k in range(K)])\n",
    "            input_, cross_attention_input = self.fuser(input_, {'description': (embeddings, attention_mask)})\n",
    "\n",
    "            out, offset_transformer = self.transformer(input_, offset_transformer, cross_attention_src=cross_attention_input, src_mask=(None))\n",
    "            print(out)\n",
    "            print()\n",
    "            break\n",
    "            \n",
    "            #Not perse true\n",
    "            prev_offset+=1\n",
    "\n",
    "        return input_, embeddings, attention_mask, tokens\n",
    "\n",
    "\n",
    "        print(sequence)\n",
    "\n",
    "        \n",
    "\n",
    "        pass\n",
    "lm_model = LMModelWrapper(projection_layer, embbedding_layer, fuser, transformer)\n",
    "lm_model(tokens=inputs['input_ids'], embeddings=torch.tensor(embeded), attention_mask = inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2546311/1139425351.py:100: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  cfg = torch.tensor([5])\n",
      "/tmp/ipykernel_2546311/1139425351.py:101: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_length = torch.tensor([256])\n",
      "/tmp/ipykernel_2546311/1139425351.py:107: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  input_ids = torch.tensor([[2048]] * self.num_codebooks)\n",
      "/tmp/ipykernel_2546311/1139425351.py:42: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
      "/tmp/ipykernel_2546311/1139425351.py:45: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_length = max_length.item()\n",
      "/tmp/ipykernel_2546311/1139425351.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_length < 2 * channel_codebooks - 1:\n",
      "/tmp/ipykernel_2546311/1139425351.py:82: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(start_ids) > 0:\n",
      "/tmp/ipykernel_2546311/1139425351.py:83: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  first_start_id = min(start_ids)\n",
      "/tmp/ipykernel_2546311/1139425351.py:83: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  first_start_id = min(start_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1435, 0.8727, 0.0470,  ..., 1.8955, 0.3980, 0.5159]],\n",
      "\n",
      "        [[1.2410, 0.8026, 0.1755,  ..., 1.9652, 0.2593, 0.6302]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "outerNode->outputs().size() == node->inputs().size() INTERNAL ASSERT FAILED at \"../torch/csrc/jit/passes/dead_code_elimination.cpp\":140, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m dummy_attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Export the model to ONNX format\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<@beartype(torch.onnx.utils.export) at 0x797f8d3e2cb0>:440\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(__beartype_object_133588732478272, __beartype_get_violation, __beartype_conf, __beartype_object_108803249437152, __beartype_object_133588799470016, __beartype_object_108803247930704, __beartype_object_108803156293248, __beartype_object_108803179589136, __beartype_getrandbits, __beartype_object_108803247923296, __beartype_object_133588737783296, __beartype_object_108803179582560, __beartype_object_133589504963328, __beartype_object_108803179810480, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m<@beartype(torch.onnx.utils._model_to_graph) at 0x797f8d3e3b50>:12\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(__beartype_object_108803248280816, __beartype_object_108803249437152, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/utils.py:1139\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1150\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m<@beartype(torch.onnx.utils._optimize_graph) at 0x797f8d3e3010>:90\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(__beartype_object_108803248280816, __beartype_get_violation, __beartype_conf, __beartype_object_108803247923296, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/utils.py:596\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GLOBALS\u001b[38;5;241m.\u001b[39mautograd_inlining:\n\u001b[1;32m    595\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_autograd_function_process(graph)\n\u001b[0;32m--> 596\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_lower_all_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# we now record some ops like ones/zeros\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# into a trace where we previously recorded constants.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# use constant prop to maintain our current level of onnx support\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# without implementing symbolics for all of them\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _disable_torch_constant_prop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: outerNode->outputs().size() == node->inputs().size() INTERNAL ASSERT FAILED at \"../torch/csrc/jit/passes/dead_code_elimination.cpp\":140, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'embeddings': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  # Output will also have variable batch size and sequence length\n",
    "    'tokens': {0: 'batch_size', 1: 'sequence_length'}  # Allow variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_tokens = inputs['input_ids']\n",
    "dummy_embeddings = torch.tensor(embeded, dtype=torch.float32)\n",
    "dummy_attention_mask = inputs['attention_mask'].to(torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    lm_model,\n",
    "    (dummy_embeddings, dummy_attention_mask, dummy_tokens),\n",
    "    f\"{folder}/test.onnx\",\n",
    "    input_names=['embeddings', 'attention_mask', 'tokens'],\n",
    "    output_names=['input_', 'embeddings', 'attention_mask', 'tokens'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder (Just using the T5 Encoder):\n",
    "optimum-cli export onnx --model google-t5/t5-base T5Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.lm.condition_provider.conditioners.description.tokenize([prompt, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession('./T5Base/encoder_model.onnx')\n",
    "\n",
    "ort_inputs = {\n",
    "    k:v.detach().numpy() for k,v in inputs.items()\n",
    "}\n",
    "\n",
    "embeded = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Name: embeddings.1\n",
      "Shape: ['embeddings.1_dim_0', 'embeddings.1_dim_1', 768]\n",
      "Type: tensor(float)\n",
      "Input Name: attention_mask.1\n",
      "Shape: ['embeddings.1_dim_0', 'embeddings.1_dim_1']\n",
      "Type: tensor(float)\n",
      "Input Name: tokens.1\n",
      "Shape: ['embeddings.1_dim_0', 'embeddings.1_dim_1']\n",
      "Type: tensor(int64)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(f'{folder}/test.onnx')\n",
    "\n",
    "for input_meta in ort_session.get_inputs():\n",
    "    print(f\"Input Name: {input_meta.name}\")\n",
    "    print(f\"Shape: {input_meta.shape}\")\n",
    "    print(f\"Type: {input_meta.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(f'{folder}/test.onnx')\n",
    "\n",
    "ort_inputs = {\n",
    "    'embeddings.1': embeded,\n",
    "    'attention_mask.1': inputs['attention_mask'].detach().numpy().astype(np.float32),\n",
    "    'tokens.1': inputs['input_ids'].detach().numpy()\n",
    "}\n",
    "\n",
    "embeded = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-0.29537773,  0.05873203, -0.02379608, ...,  0.00863934,\n",
       "          -0.00691605, -0.00769234]],\n",
       " \n",
       "        [[-0.29537773,  0.05873203, -0.02379608, ...,  0.00863934,\n",
       "          -0.00691605, -0.00769234]]], dtype=float32),\n",
       " array([[[ 0.31444335,  0.16241327,  0.11668988, ...,  0.40329838,\n",
       "           0.34918803,  0.5680022 ],\n",
       "         [-0.09240554,  0.22058195,  0.64709044, ..., -0.2670372 ,\n",
       "           0.6926601 ,  1.001211  ],\n",
       "         [-0.24913806, -0.40614206,  0.08446971, ..., -0.21988793,\n",
       "          -0.7242352 ,  0.63619673],\n",
       "         ...,\n",
       "         [ 0.17368768,  0.34110174, -0.2478149 , ...,  0.17052141,\n",
       "           0.12066975,  0.02595249],\n",
       "         [ 0.4187473 ,  0.22387516,  0.10361063, ...,  0.12866525,\n",
       "           0.2915484 , -0.68793195],\n",
       "         [-0.00938433, -0.00760569, -0.02425367, ...,  0.00280249,\n",
       "          -0.01758854, -0.0108365 ]],\n",
       " \n",
       "        [[-0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "          -0.        ,  0.        ],\n",
       "         [-0.        ,  0.        , -0.        , ...,  0.        ,\n",
       "          -0.        , -0.        ],\n",
       "         [-0.        ,  0.        , -0.        , ...,  0.        ,\n",
       "          -0.        , -0.        ],\n",
       "         ...,\n",
       "         [-0.        ,  0.        , -0.        , ...,  0.        ,\n",
       "          -0.        , -0.        ],\n",
       "         [-0.        ,  0.        , -0.        , ...,  0.        ,\n",
       "          -0.        , -0.        ],\n",
       "         [-0.        ,  0.        , -0.        , ...,  0.        ,\n",
       "          -0.        , -0.        ]]], dtype=float32),\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([[ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,\n",
       "            11, 13353,     1],\n",
       "        [    1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]], dtype=int64)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
