{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trick to make the model actually function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration, MusicgenModel\n",
    "\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "x = model.config.to_dict()\n",
    "x['decoder']['num_codebooks'] = 4\n",
    "model.config = model.config.from_dict(x)\n",
    "# model.save_pretrained(\"musicgen_fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/MUSICGEN/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bas/anaconda3/envs/MUSICGEN/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/bas/anaconda3/envs/MUSICGEN/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then export the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from optimum.exporters.onnx.model_configs import MusicgenOnnxConfig\n",
    "from transformers import MusicgenConfig\n",
    "\n",
    "model_id = \"musicgen_fixed\"\n",
    "\n",
    "main_export(\n",
    "    model_id,\n",
    "    output=\"musicgen-stereo\",\n",
    "    task='text-to-audio'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli onnxruntime quantize --avx512 --onnx_model musicgen-stereo -o quantized_musicgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import json\n",
    "\n",
    "# Load the ORT config\n",
    "with open(\"./quantized_musicgen/ort_config.json\", \"r\") as f:\n",
    "    ort_config = json.load(f)\n",
    "\n",
    "# Apply ORT configuration when initializing the session\n",
    "session_options = ort.SessionOptions()\n",
    "if \"graph_optimization_level\" in ort_config:\n",
    "    session_options.graph_optimization_level = ort_config[\"graph_optimization_level\"]\n",
    "\n",
    "# Example: Setting execution providers, thread counts, etc.\n",
    "if \"execution_providers\" in ort_config:\n",
    "    session_options.execution_mode = ort_config[\"execution_providers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AddedToken\n",
    "\n",
    "# Load tokenizer configuration and special tokens map\n",
    "with open(\"./quantized_musicgen/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "\n",
    "with open(\"./quantized_musicgen/special_tokens_map.json\", \"r\") as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "    for key, value in special_tokens_map.items():\n",
    "        if key != 'additional_special_tokens':\n",
    "            special_tokens_map[key] = AddedToken(\n",
    "                content = value['content'], \n",
    "                single_word = value['single_word'], \n",
    "                lstrip = value['lstrip'], \n",
    "                rstrip = value['rstrip'], \n",
    "                special = True, \n",
    "                normalized = value['normalized']\n",
    "            )\n",
    "\n",
    "# Load the model configuration (config.json)\n",
    "with open(\"./quantized_musicgen/config.json\", \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Load the tokenizer with configuration\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./quantized_musicgen/tokenizer.json\")\n",
    "\n",
    "# Add the special tokens from the special_tokens_map.json\n",
    "tokenizer.add_special_tokens(special_tokens_map)\n",
    "\n",
    "# Configure tokenizer with settings from tokenizer_config.json\n",
    "if \"padding_side\" in tokenizer_config:\n",
    "    print('adding padding_side')\n",
    "    tokenizer.padding_side = tokenizer_config[\"padding_side\"]\n",
    "if \"truncation_side\" in tokenizer_config:\n",
    "    print('adding truncation_side')\n",
    "    tokenizer.truncation_side = tokenizer_config[\"truncation_side\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_session = ort.InferenceSession('./quantized_musicgen/text_encoder_quantized.onnx', sess_options=session_options)\n",
    "decoder_session = ort.InferenceSession('./quantized_musicgen/decoder_model_quantized.onnx', sess_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"80s pop track with bassy drums and synth\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference for text encoding\n",
    "encoded_text = text_encoder_session.run(None, {\n",
    "    'input_ids': inputs['input_ids'],\n",
    "    'attention_mask': inputs['attention_mask']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,\n",
       "           11, 13353,     1],\n",
       "       [ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,\n",
       "           11, 13353,     1],\n",
       "       [ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,\n",
       "           11, 13353,     1],\n",
       "       [ 2775,     7,  2783,  1463,    28,  7981,    63,  5253,     7,\n",
       "           11, 13353,     1]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.repeat(inputs['input_ids'], repeats=4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': '',\n",
       " 'activation_dropout': 0.0,\n",
       " 'activation_function': 'gelu',\n",
       " 'add_cross_attention': False,\n",
       " 'architectures': None,\n",
       " 'attention_dropout': 0.0,\n",
       " 'audio_channels': 2,\n",
       " 'bad_words_ids': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'bos_token_id': 2048,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'do_sample': False,\n",
       " 'dropout': 0.1,\n",
       " 'early_stopping': False,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'eos_token_id': None,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'ffn_dim': 4096,\n",
       " 'finetuning_task': None,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'hidden_size': 1024,\n",
       " 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'},\n",
       " 'initializer_factor': 0.02,\n",
       " 'is_decoder': False,\n",
       " 'is_encoder_decoder': False,\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'layerdrop': 0.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'max_length': 20,\n",
       " 'max_position_embeddings': 2048,\n",
       " 'min_length': 0,\n",
       " 'model_type': 'musicgen_decoder',\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'num_attention_heads': 16,\n",
       " 'num_beam_groups': 1,\n",
       " 'num_beams': 1,\n",
       " 'num_codebooks': 4,\n",
       " 'num_hidden_layers': 24,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'pad_token_id': 2048,\n",
       " 'prefix': None,\n",
       " 'problem_type': None,\n",
       " 'pruned_heads': {},\n",
       " 'remove_invalid_values': False,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'return_dict': True,\n",
       " 'return_dict_in_generate': False,\n",
       " 'scale_embedding': False,\n",
       " 'sep_token_id': None,\n",
       " 'suppress_tokens': None,\n",
       " 'task_specific_params': None,\n",
       " 'temperature': 1.0,\n",
       " 'tf_legacy_loss': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_class': None,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'torch_dtype': None,\n",
       " 'torchscript': False,\n",
       " 'typical_p': 1.0,\n",
       " 'use_bfloat16': False,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 2048}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-09-27 03:22:47.168206832 [E:onnxruntime:, sequential_executor.cc:516 ExecuteKernel] Non-zero status code returned while running Gather node. Name:'/decoder/model/decoder/embed_tokens.1/Gather' Status Message: indices element out of data bounds, idx=2775 must be within the inclusive range [-2049,2048]\u001b[m\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/decoder/model/decoder/embed_tokens.1/Gather' Status Message: indices element out of data bounds, idx=2775 must be within the inclusive range [-2049,2048]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m decoder_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrepeat(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_hidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m: encoded_text[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate output from the decoder\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MUSICGEN/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/decoder/model/decoder/embed_tokens.1/Gather' Status Message: indices element out of data bounds, idx=2775 must be within the inclusive range [-2049,2048]"
     ]
    }
   ],
   "source": [
    "# Process output and run decoder (adjusted based on model config)\n",
    "decoder_inputs = {\n",
    "    'input_ids': np.repeat(inputs['input_ids'], repeats=4, axis=0),\n",
    "    'encoder_hidden_states': encoded_text[0],\n",
    "    'encoder_attention_mask': inputs['attention_mask']\n",
    "}\n",
    "\n",
    "# Generate output from the decoder\n",
    "decoder_output = decoder_session.run(None, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('./quantized_musicgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of decoder layers (in your case, 24 for Musicgen)\n",
    "num_layers = 24\n",
    "\n",
    "# Assuming hidden_size is the dimension of the model (1024 for Musicgen)\n",
    "hidden_size = 1024\n",
    "\n",
    "# Batch size, number of heads, sequence length (1 for the first step), and attention head size\n",
    "batch_size = 1\n",
    "num_heads = 16  # This depends on your model configuration\n",
    "sequence_length = 1\n",
    "head_size = hidden_size // num_heads\n",
    "\n",
    "# Create past_key_values as a list of zero tensors for each layer\n",
    "past_key_values = []\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    decoder_key = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    decoder_value = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    encoder_key = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    encoder_value = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    past_key_values.append({\n",
    "        \"decoder.key\": decoder_key,\n",
    "        \"decoder.value\": decoder_value,\n",
    "        \"encoder.key\": encoder_key,\n",
    "        \"encoder.value\": encoder_value,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states[:,:3,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "generated_tokens = decoder_input_ids\n",
    "use_cache_branch = np.array([False], dtype=bool)  # Use False for first step\n",
    "\n",
    "for step in range(gen_config.max_length):\n",
    "    # Prepare the input dictionary for the ONNX session\n",
    "    inputs = {\n",
    "        \"input_ids\": decoder_input_ids,\n",
    "        \"encoder_hidden_states\": encoder_hidden_states,\n",
    "        \"encoder_attention_mask\": input_tokens['attention_mask'],\n",
    "        \"use_cache_branch\": np.array([False], dtype=bool),  # Set to True to use past key values\n",
    "    }\n",
    "\n",
    "    # Add past key values to the input\n",
    "    for i, layer_past in enumerate(past_key_values):\n",
    "        inputs[f\"past_key_values.{i}.decoder.key\"] = layer_past[\"decoder.key\"]\n",
    "        inputs[f\"past_key_values.{i}.decoder.value\"] = layer_past[\"decoder.value\"]\n",
    "        inputs[f\"past_key_values.{i}.encoder.key\"] = layer_past[\"encoder.key\"]\n",
    "        inputs[f\"past_key_values.{i}.encoder.value\"] = layer_past[\"encoder.value\"]\n",
    "\n",
    "    # Run the ONNX session\n",
    "    decoder_outputs = decoder_session.run(None, inputs)\n",
    "\n",
    "    \n",
    "    # Get logits and past key values\n",
    "    logits = decoder_outputs[0]\n",
    "    # Extract past_key_values from decoder_outputs if they are present\n",
    "    \n",
    "    # Sample next token (using greedy search, beam search, or sampling)\n",
    "    next_token_id = np.argmax(logits[:, -1, :], axis=-1).reshape(4, 1)\n",
    "    \n",
    "    # Append the next token to generated tokens\n",
    "    generated_tokens = np.concatenate([generated_tokens, next_token_id], axis=1)\n",
    "    \n",
    "    # Update inputs for next step\n",
    "    use_cache_branch = np.array([True], dtype=bool)\n",
    "    # Update past_key_values for next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for encodec decoder\n",
    "encodec_inputs = {\n",
    "    \"codes\": generated_tokens  # Ensure this matches the expected input shape\n",
    "}\n",
    "\n",
    "# Run the encodec decoder\n",
    "audio_outputs = encodec_decoder_session.run(None, encodec_inputs)\n",
    "\n",
    "# Get the audio waveform\n",
    "audio_waveform = audio_outputs[0]  # Adjust index based on actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write('generated_audio.wav', audio_waveform.squeeze(), samplerate=gen_config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_meta in decoder_session.get_inputs():\n",
    "    print(f\"Input name: {input_meta.name}, shape: {input_meta.shape}, type: {input_meta.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14.00       9.00      13.00      11.30       8.25      11.40      14.00      13.40      14.00      10.97       8.13      10.65\n",
      "      9.00      13.00      12.00       8.25      10.15       9.85      13.00      13.00      13.00       8.14      10.41       9.35\n",
      "     13.00      12.00      17.00      11.40       9.85      13.90      16.93      16.49      17.00      11.16      10.17      13.47\n",
      "     11.30       8.25      11.40       9.98       7.96      10.69      13.94      13.48      13.93       9.90       7.94      10.22\n",
      "      8.25      10.15       9.85       7.96       9.11       9.18      12.96      12.76      12.95       8.02       9.19       8.84\n",
      "     11.40       9.85      13.90      10.69       9.18      12.79      16.89      16.42      16.89      10.70       9.38      12.46\n",
      "     14.00      13.00      16.93      13.94      12.96      16.89      24.65      23.97      24.59      14.30      13.22      16.47\n",
      "     13.40      13.00      16.49      13.48      12.76      16.42      23.97      23.43      23.90      13.82      13.07      16.02\n",
      "     14.00      13.00      17.00      13.93      12.95      16.89      24.59      23.90      24.55      14.27      13.23      16.47\n",
      "     10.97       8.14      11.16       9.90       8.02      10.70      14.30      13.82      14.27       9.98       7.98      10.30\n",
      "      8.13      10.41      10.17       7.94       9.19       9.38      13.22      13.07      13.23       7.98       9.55       9.11\n",
      "     10.65       9.35      13.47      10.22       8.84      12.46      16.47      16.02      16.47      10.30       9.11      12.37\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeros((len(vecs), len(vecs[0])))\n",
    "for i in range(len(vecs)):\n",
    "    matrix[i, :] = vecs[i]\n",
    "matrix = np.dot(matrix,matrix.T)\n",
    "for row in matrix:\n",
    "    print(\" \".join(f\"{value:10.2f}\" for value in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.36       0.06       0.30       0.20       0.02       0.21       0.36       0.33       0.36       0.18       0.01       0.16\n",
      "      0.06       0.30       0.24       0.02       0.13       0.11       0.30       0.30       0.30       0.01       0.15       0.08\n",
      "      0.30       0.24       0.54       0.21       0.11       0.36       0.54       0.51       0.54       0.19       0.13       0.33\n",
      "      0.20       0.02       0.21       0.12       0.00       0.16       0.36       0.33       0.36       0.12       0.00       0.14\n",
      "      0.02       0.13       0.11       0.00       0.07       0.07       0.30       0.29       0.30       0.00       0.07       0.05\n",
      "      0.21       0.11       0.36       0.16       0.07       0.29       0.54       0.51       0.54       0.17       0.09       0.27\n",
      "      0.36       0.30       0.54       0.36       0.30       0.54       1.00       0.96       1.00       0.38       0.32       0.51\n",
      "      0.33       0.30       0.51       0.33       0.29       0.51       0.96       0.93       0.96       0.35       0.31       0.48\n",
      "      0.36       0.30       0.54       0.36       0.30       0.54       1.00       0.96       0.99       0.38       0.32       0.51\n",
      "      0.18       0.01       0.19       0.12       0.00       0.17       0.38       0.35       0.38       0.12       0.00       0.14\n",
      "      0.01       0.15       0.13       0.00       0.07       0.09       0.32       0.31       0.32       0.00       0.10       0.07\n",
      "      0.16       0.08       0.33       0.14       0.05       0.27       0.51       0.48       0.51       0.14       0.07       0.26\n"
     ]
    }
   ],
   "source": [
    "dfmax, dfmin = matrix.max(), matrix.min()\n",
    "\n",
    "matrix = (matrix - dfmin)/(dfmax - dfmin)\n",
    "for row in matrix:\n",
    "    print(\" \".join(f\"{value:10.2f}\" for value in row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
