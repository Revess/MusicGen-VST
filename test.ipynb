{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trick to make the model actually function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration, MusicgenModel\n",
    "\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "x = model.config.to_dict()\n",
    "x['decoder']['num_codebooks'] = 4\n",
    "model.config = model.config.from_dict(x)\n",
    "model.save_pretrained(\"musicgen_fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then export the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of the model checkpoint at musicgen_fixed were not used when initializing MusicgenForConditionalGeneration: ['decoder.lm_heads.4.weight', 'decoder.lm_heads.5.weight', 'decoder.lm_heads.6.weight', 'decoder.lm_heads.7.weight', 'decoder.model.decoder.embed_tokens.4.weight', 'decoder.model.decoder.embed_tokens.5.weight', 'decoder.model.decoder.embed_tokens.6.weight', 'decoder.model.decoder.embed_tokens.7.weight']\n",
      "- This IS expected if you are initializing MusicgenForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MusicgenForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using the export variant text-conditional-with-past. Available variants are:\n",
      "    - text-conditional-with-past: Exports Musicgen to ONNX to generate audio samples conditioned on a text prompt (Reference: https://huggingface.co/docs/transformers/model_doc/musicgen#text-conditional-generation). This uses the decoder KV cache. The following subcomponents are exported:\n",
      "\t\t* text_encoder.onnx: corresponds to the text encoder part in https://github.com/huggingface/transformers/blob/v4.39.1/src/transformers/models/musicgen/modeling_musicgen.py#L1457.\n",
      "\t\t* encodec_decode.onnx: corresponds to the Encodec audio encoder part in https://github.com/huggingface/transformers/blob/v4.39.1/src/transformers/models/musicgen/modeling_musicgen.py#L2472-L2480.\n",
      "\t\t* decoder_model.onnx: The Musicgen decoder, without past key values input, and computing cross attention. Not required at inference (use decoder_model_merged.onnx instead).\n",
      "\t\t* decoder_with_past_model.onnx: The Musicgen decoder, with past_key_values input (KV cache filled), not computing cross attention. Not required at inference (use decoder_model_merged.onnx instead).\n",
      "\t\t* decoder_model_merged.onnx: The two previous models fused in one, to avoid duplicating weights. A boolean input `use_cache_branch` allows to select the branch to use. In the first forward pass where the KV cache is empty, dummy past key values inputs need to be passed and are ignored with use_cache_branch=False.\n",
      "\t\t* build_delay_pattern_mask.onnx: A model taking as input `input_ids`, `pad_token_id`, `max_length`, and building a delayed pattern mask to the input_ids. Implements https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/models/musicgen/modeling_musicgen.py#L1054.\n",
      "\n",
      "***** Exporting submodel 1/5: T5EncoderModel *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "\n",
      "***** Exporting submodel 2/5: EncodecModel *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/optimum/exporters/onnx/model_patcher.py:942: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(audio_codes) != 1:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:433: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  quantized_out = torch.tensor(0.0, device=codes.device)\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:434: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, indices in enumerate(codes):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:144: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_pad = max(padding_left, padding_right)\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if length <= max_pad:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "\n",
      "***** Exporting submodel 3/5: MusicgenForConditionalGeneration *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.weights.size(0):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:278: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "\n",
      "***** Exporting submodel 4/5: MusicgenForConditionalGeneration *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:230: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (\n",
      "\n",
      "***** Exporting submodel 5/5: MusicgenForCausalLM *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/optimum/exporters/onnx/model_patcher.py:851: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_length < 2 * channel_codebooks - 1:\n",
      "Post-processing the exported models...\n",
      "The two models proto have different outputs (97 and 49 outputs). Constant outputs will be added to unify the two models outputs. This is expected for encoder-decoder models where cached cross-attention key/values are constant outputs, omitted in the model with KV cache.\n",
      "Adding a constant output for present.0.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.0.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.1.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.1.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.2.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.2.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.3.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.3.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.4.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.4.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.5.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.5.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.6.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.6.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.7.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.7.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.8.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.8.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.9.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.9.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.10.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.10.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.11.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.11.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.12.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.12.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.13.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.13.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.14.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.14.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.15.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.15.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.16.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.16.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.17.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.17.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.18.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.18.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.19.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.19.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.20.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.20.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.21.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.21.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.22.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.22.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.23.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Adding a constant output for present.23.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "\n",
      "Validating ONNX model musicgen-stereo/text_encoder.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (last_hidden_state)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (2, 16, 768) matches (2, 16, 768)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\n",
      "Validating ONNX model musicgen-stereo/encodec_decode.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (audio_values)\n",
      "\t- Validating ONNX Model output \"audio_values\":\n",
      "\t\t-[✓] (2, 1, 10240) matches (2, 1, 10240)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\n",
      "Validating ONNX model musicgen-stereo/decoder_model_merged.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (present.20.decoder.value, present.5.encoder.value, present.2.decoder.value, present.5.encoder.key, present.15.decoder.key, present.9.decoder.value, present.14.encoder.value, present.12.encoder.value, present.20.encoder.key, present.12.encoder.key, present.7.decoder.value, present.8.encoder.value, present.11.decoder.value, present.2.encoder.value, present.17.decoder.value, present.0.decoder.key, present.7.decoder.key, present.1.encoder.value, present.2.encoder.key, present.4.decoder.value, present.4.decoder.key, present.16.decoder.value, present.11.decoder.key, present.5.decoder.key, present.15.encoder.key, present.9.encoder.key, present.11.encoder.key, present.6.decoder.key, present.13.encoder.key, present.1.decoder.key, present.22.encoder.value, present.10.decoder.value, present.14.encoder.key, present.4.encoder.key, present.14.decoder.key, present.3.decoder.value, present.13.encoder.value, present.7.encoder.value, present.18.decoder.value, present.7.encoder.key, present.8.encoder.key, present.19.encoder.key, present.22.decoder.value, present.5.decoder.value, present.9.encoder.value, present.23.encoder.key, present.16.encoder.value, present.21.decoder.value, present.18.encoder.value, present.21.decoder.key, present.15.decoder.value, present.23.decoder.value, present.19.decoder.key, present.10.encoder.value, present.2.decoder.key, present.3.encoder.value, present.6.encoder.value, present.14.decoder.value, present.13.decoder.key, present.23.encoder.value, present.10.encoder.key, present.4.encoder.value, present.13.decoder.value, present.23.decoder.key, present.0.encoder.value, present.12.decoder.key, present.6.encoder.key, present.16.decoder.key, present.21.encoder.value, present.0.encoder.key, present.12.decoder.value, present.18.decoder.key, present.6.decoder.value, present.15.encoder.value, present.1.decoder.value, present.18.encoder.key, present.22.encoder.key, present.20.encoder.value, present.11.encoder.value, present.8.decoder.value, logits, present.16.encoder.key, present.1.encoder.key, present.0.decoder.value, present.17.encoder.key, present.8.decoder.key, present.10.decoder.key, present.19.decoder.value, present.3.encoder.key, present.21.encoder.key, present.20.decoder.key, present.19.encoder.value, present.22.decoder.key, present.3.decoder.key, present.9.decoder.key, present.17.encoder.value, present.17.decoder.key)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (8, 16, 2048) matches (8, 16, 2048)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\n",
      "Validating ONNX model musicgen-stereo/decoder_model_merged.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (present.20.decoder.value, present.11.decoder.key, present.2.decoder.value, present.12.decoder.key, present.15.decoder.key, present.5.decoder.value, present.5.decoder.key, present.9.decoder.value, present.16.decoder.key, present.21.decoder.value, present.6.decoder.key, present.12.decoder.value, present.1.decoder.key, present.18.decoder.key, present.21.decoder.key, present.15.decoder.value, present.23.decoder.value, present.6.decoder.value, present.10.decoder.value, present.1.decoder.value, present.19.decoder.key, present.2.decoder.key, present.7.decoder.value, present.11.decoder.value, present.14.decoder.key, present.3.decoder.value, present.17.decoder.value, present.8.decoder.value, logits, present.0.decoder.key, present.7.decoder.key, present.14.decoder.value, present.13.decoder.key, present.0.decoder.value, present.18.decoder.value, present.19.decoder.value, present.8.decoder.key, present.10.decoder.key, present.4.decoder.value, present.4.decoder.key, present.20.decoder.key, present.16.decoder.value, present.13.decoder.value, present.22.decoder.key, present.22.decoder.value, present.3.decoder.key, present.23.decoder.key, present.9.decoder.key, present.17.decoder.key)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (8, 1, 2048) matches (8, 1, 2048)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\n",
      "Validating ONNX model musicgen-stereo/build_delay_pattern_mask.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (input_ids_edited, delay_pattern_mask)\n",
      "\t- Validating ONNX Model output \"input_ids_edited\":\n",
      "\t\t-[✓] (8, 16) matches (8, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"delay_pattern_mask\":\n",
      "\t\t-[✓] (8, 21) matches (8, 21)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "The ONNX export succeeded and the exported model was saved at: musicgen-stereo\n"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from optimum.exporters.onnx.model_configs import MusicgenOnnxConfig\n",
    "from transformers import MusicgenConfig\n",
    "\n",
    "model_id = \"musicgen_fixed\"\n",
    "\n",
    "main_export(\n",
    "    model_id,\n",
    "    output=\"musicgen-stereo\",\n",
    "    task='text-to-audio'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/NonZero_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__121\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"batch_size\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 4\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"chunk_length\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.0/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__3\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.4/block.1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__26\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.4/block.3/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__38\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.7/block.1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__56\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.7/block.3/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__68\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.10/block.1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__86\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.10/block.3/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__98\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.13/block.1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__116\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.13/block.3/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__128\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/decoder/layers.15/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__142\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_musicgen (external data format: False)\n",
      "Configuration saved in quantized_musicgen/ort_config.json\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli onnxruntime quantize --avx512 --onnx_model musicgen-stereo -o quantized_musicgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './quantized_musicgen/ort_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the ORT config\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./quantized_musicgen/ort_config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     ort_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Apply ORT configuration when initializing the session\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './quantized_musicgen/ort_config.json'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import json\n",
    "\n",
    "# Load the ORT config\n",
    "with open(\"./quantized_musicgen/ort_config.json\", \"r\") as f:\n",
    "    ort_config = json.load(f)\n",
    "\n",
    "# Apply ORT configuration when initializing the session\n",
    "session_options = ort.SessionOptions()\n",
    "if \"graph_optimization_level\" in ort_config:\n",
    "    session_options.graph_optimization_level = ort_config[\"graph_optimization_level\"]\n",
    "\n",
    "# Example: Setting execution providers, thread counts, etc.\n",
    "if \"execution_providers\" in ort_config:\n",
    "    session_options.execution_mode = ort_config[\"execution_providers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AddedToken\n",
    "\n",
    "# Load tokenizer configuration and special tokens map\n",
    "with open(\"./quantized_musicgen/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "\n",
    "with open(\"./quantized_musicgen/special_tokens_map.json\", \"r\") as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "    for key, value in special_tokens_map.items():\n",
    "        if key != 'additional_special_tokens':\n",
    "            special_tokens_map[key] = AddedToken(\n",
    "                content = value['content'], \n",
    "                single_word = value['single_word'], \n",
    "                lstrip = value['lstrip'], \n",
    "                rstrip = value['rstrip'], \n",
    "                special = True, \n",
    "                normalized = value['normalized']\n",
    "            )\n",
    "\n",
    "# Load the model configuration (config.json)\n",
    "with open(\"./quantized_musicgen/config.json\", \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Load the tokenizer with configuration\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./quantized_musicgen/tokenizer.json\")\n",
    "\n",
    "# Add the special tokens from the special_tokens_map.json\n",
    "tokenizer.add_special_tokens(special_tokens_map)\n",
    "\n",
    "# Configure tokenizer with settings from tokenizer_config.json\n",
    "if \"padding_side\" in tokenizer_config:\n",
    "    print('adding padding_side')\n",
    "    tokenizer.padding_side = tokenizer_config[\"padding_side\"]\n",
    "if \"truncation_side\" in tokenizer_config:\n",
    "    print('adding truncation_side')\n",
    "    tokenizer.truncation_side = tokenizer_config[\"truncation_side\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_session = ort.InferenceSession('./quantized_musicgen/text_encoder_quantized.onnx', sess_options=session_options)\n",
    "decoder_session = ort.InferenceSession('./quantized_musicgen/decoder_model_quantized.onnx', sess_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"80s pop track with bassy drums and synth\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference for text encoding\n",
    "encoded_text = text_encoder_session.run(None, {\n",
    "    'input_ids': inputs['input_ids'],\n",
    "    'attention_mask': inputs['attention_mask']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.repeat(inputs['input_ids'], repeats=4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process output and run decoder (adjusted based on model config)\n",
    "decoder_inputs = {\n",
    "    'input_ids': np.repeat(inputs['input_ids'], repeats=4, axis=0),\n",
    "    'encoder_hidden_states': encoded_text[0],\n",
    "    'encoder_attention_mask': inputs['attention_mask']\n",
    "}\n",
    "\n",
    "# Generate output from the decoder\n",
    "decoder_output = decoder_session.run(None, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('./quantized_musicgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of decoder layers (in your case, 24 for Musicgen)\n",
    "num_layers = 24\n",
    "\n",
    "# Assuming hidden_size is the dimension of the model (1024 for Musicgen)\n",
    "hidden_size = 1024\n",
    "\n",
    "# Batch size, number of heads, sequence length (1 for the first step), and attention head size\n",
    "batch_size = 1\n",
    "num_heads = 16  # This depends on your model configuration\n",
    "sequence_length = 1\n",
    "head_size = hidden_size // num_heads\n",
    "\n",
    "# Create past_key_values as a list of zero tensors for each layer\n",
    "past_key_values = []\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    decoder_key = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    decoder_value = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    encoder_key = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    encoder_value = np.zeros((batch_size, num_heads, sequence_length, head_size), dtype=np.float32)\n",
    "    past_key_values.append({\n",
    "        \"decoder.key\": decoder_key,\n",
    "        \"decoder.value\": decoder_value,\n",
    "        \"encoder.key\": encoder_key,\n",
    "        \"encoder.value\": encoder_value,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states[:,:3,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "generated_tokens = decoder_input_ids\n",
    "use_cache_branch = np.array([False], dtype=bool)  # Use False for first step\n",
    "\n",
    "for step in range(gen_config.max_length):\n",
    "    # Prepare the input dictionary for the ONNX session\n",
    "    inputs = {\n",
    "        \"input_ids\": decoder_input_ids,\n",
    "        \"encoder_hidden_states\": encoder_hidden_states,\n",
    "        \"encoder_attention_mask\": input_tokens['attention_mask'],\n",
    "        \"use_cache_branch\": np.array([False], dtype=bool),  # Set to True to use past key values\n",
    "    }\n",
    "\n",
    "    # Add past key values to the input\n",
    "    for i, layer_past in enumerate(past_key_values):\n",
    "        inputs[f\"past_key_values.{i}.decoder.key\"] = layer_past[\"decoder.key\"]\n",
    "        inputs[f\"past_key_values.{i}.decoder.value\"] = layer_past[\"decoder.value\"]\n",
    "        inputs[f\"past_key_values.{i}.encoder.key\"] = layer_past[\"encoder.key\"]\n",
    "        inputs[f\"past_key_values.{i}.encoder.value\"] = layer_past[\"encoder.value\"]\n",
    "\n",
    "    # Run the ONNX session\n",
    "    decoder_outputs = decoder_session.run(None, inputs)\n",
    "\n",
    "    \n",
    "    # Get logits and past key values\n",
    "    logits = decoder_outputs[0]\n",
    "    # Extract past_key_values from decoder_outputs if they are present\n",
    "    \n",
    "    # Sample next token (using greedy search, beam search, or sampling)\n",
    "    next_token_id = np.argmax(logits[:, -1, :], axis=-1).reshape(4, 1)\n",
    "    \n",
    "    # Append the next token to generated tokens\n",
    "    generated_tokens = np.concatenate([generated_tokens, next_token_id], axis=1)\n",
    "    \n",
    "    # Update inputs for next step\n",
    "    use_cache_branch = np.array([True], dtype=bool)\n",
    "    # Update past_key_values for next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for encodec decoder\n",
    "encodec_inputs = {\n",
    "    \"codes\": generated_tokens  # Ensure this matches the expected input shape\n",
    "}\n",
    "\n",
    "# Run the encodec decoder\n",
    "audio_outputs = encodec_decoder_session.run(None, encodec_inputs)\n",
    "\n",
    "# Get the audio waveform\n",
    "audio_waveform = audio_outputs[0]  # Adjust index based on actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write('generated_audio.wav', audio_waveform.squeeze(), samplerate=gen_config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_meta in decoder_session.get_inputs():\n",
    "    print(f\"Input name: {input_meta.name}, shape: {input_meta.shape}, type: {input_meta.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(vecs), len(vecs[0])))\n",
    "for i in range(len(vecs)):\n",
    "    matrix[i, :] = vecs[i]\n",
    "matrix = np.dot(matrix,matrix.T)\n",
    "for row in matrix:\n",
    "    print(\" \".join(f\"{value:10.2f}\" for value in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmax, dfmin = matrix.max(), matrix.min()\n",
    "\n",
    "matrix = (matrix - dfmin)/(dfmax - dfmin)\n",
    "for row in matrix:\n",
    "    print(\" \".join(f\"{value:10.2f}\" for value in row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
