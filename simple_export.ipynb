{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch, os, glob, json, math\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import inspect\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "\n",
    "folder = './musicgen-stereo-small'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth asd asd asd\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "res = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=259, temperature=2.0, top_k=500, top_p=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try and export the full model\n",
    "\n",
    "Doesnt work bc of problems in attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenWrapper(nn.Module):\n",
    "    def __init__(self, model: MusicgenForConditionalGeneration):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, guidance_scale=3, max_new_tokens=256, temperature=2.0, top_k=500, top_p=0.0):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        output_values = model.generate(**inputs, guidance_scale=guidance_scale.item(), max_new_tokens=max_new_tokens.item(), temperature=temperature.item(), top_k=top_k.item(), top_p=top_p.item())\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musicgen_wrapper = MusicGenWrapper(model)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_guidance_scale = torch.tensor(3, dtype=torch.int64)\n",
    "dummy_max_new_tokens = torch.tensor(256, dtype=torch.int64)\n",
    "dummy_temperature = torch.tensor(2.0, dtype=torch.float32)\n",
    "dummy_top_k = torch.tensor(500, dtype=torch.int64)\n",
    "dummy_top_p = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    musicgen_wrapper,\n",
    "    (inputs['input_ids'], inputs['attention_mask'], dummy_guidance_scale, dummy_max_new_tokens, dummy_temperature, dummy_top_k, dummy_top_p),\n",
    "    f\"{folder}/musicgen.onnx\", \n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'attention_mask',\n",
    "        'guidance_scale',\n",
    "        'max_new_tokens',\n",
    "        'temperature',\n",
    "        'top_k',\n",
    "        'top_p'\n",
    "    ],\n",
    "    output_names=['output_values'],\n",
    "    dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model\n",
    "TODO:\n",
    "- [x] Configuration lists exported\n",
    "- [x] Text encoder exported\n",
    "- [x] Projection layer exported\n",
    "- [x] Decoder layer exported\n",
    "- [ ] Sampling function exported\n",
    "- [x] Output decoder exported\n",
    "- [ ] Look at making layers efficient\n",
    "- [ ] Full model throughput test\n",
    "- [ ] Research way to export the sample input version\n",
    "\n",
    "Flow:                                                           This will be in a forloop\n",
    "tokenized inputs and mask -> Text Encoder -> PreProcess -> [Decoder -> Sample] -> Audio Encoder -> Wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.save_pretrained(f'{folder}')\n",
    "processor.save_pretrained(f'{folder}')\n",
    "model.config.to_json_file(f'{folder}/config.json')\n",
    "model.generation_config.to_json_file(f'{folder}/generation_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderWrapper(nn.Module):\n",
    "    def __init__(self, text_encoder):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cfg=None):\n",
    "        last_hidden_state = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        if cfg is not None:\n",
    "            cfg_tensor = cfg.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "            condition = (cfg_tensor > 1).float()  # Create a condition tensor\n",
    "            if condition: # This enforces the addition of cfg as a variable\n",
    "                last_hidden_state = condition * torch.concatenate([last_hidden_state, torch.zeros_like(last_hidden_state)], dim=0)\n",
    "                res_attention_mask = condition * torch.concatenate([attention_mask, torch.zeros_like(attention_mask)], dim=0)\n",
    "        else:\n",
    "            res_attention_mask = attention_mask\n",
    "\n",
    "        return last_hidden_state, res_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_wrapper = TextEncoderWrapper(model.text_encoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoded': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_input_ids = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "dummy_attention_mask = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "dummy_cfg = torch.tensor(3, dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    text_encoder_wrapper,\n",
    "    (dummy_input_ids, dummy_attention_mask, dummy_cfg),\n",
    "    f\"{folder}/text_encoder.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask', 'cfg'],\n",
    "    output_names=['last_hidden_state', 'res_attention_mask'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the projection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.past_key_values = None\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states, encoder_attention_mask):\n",
    "        outputs = self.decoder(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = None,\n",
    "            encoder_hidden_states = encoder_hidden_states,\n",
    "            encoder_attention_mask = encoder_attention_mask,\n",
    "            head_mask = None,\n",
    "            cross_attn_head_mask = None,\n",
    "            past_key_values = self.past_key_values,\n",
    "            inputs_embeds = None,\n",
    "            labels = None,\n",
    "            use_cache = True,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            return_dict = True\n",
    "        )\n",
    "\n",
    "        self.past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_wrapper = DecoderWrapper(model.decoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size'},\n",
    "    'encoder_hidden_states': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'encoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_input_ids = torch.randint(0, 100, (16, 1), dtype=torch.int64)\n",
    "dummy_encoder_hidden_states = torch.randn((2, 18, 1024), dtype=torch.float32)\n",
    "dummy_encoder_attention_mask = torch.randint(0, 100, (2, 18), dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    decoder_wrapper,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_encoder_hidden_states,\n",
    "        dummy_encoder_attention_mask,\n",
    "    ),\n",
    "    f\"{folder}/decoder.onnx\",\n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'encoder_hidden_states',\n",
    "        'encoder_attention_mask',\n",
    "    ],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the audio_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a PT wrapper for the decoding portion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeAudioWrapper(nn.Module):\n",
    "    def __init__(self, audio_encoder):\n",
    "        super().__init__()\n",
    "        self.audio_encoder = audio_encoder\n",
    "\n",
    "    def apply_delay_pattern_mask(self, input_ids, decoder_pad_token_mask):\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        decoder_pad_token_mask = decoder_pad_token_mask[..., :seq_len]\n",
    "        input_ids = torch.where(decoder_pad_token_mask == -1, input_ids, decoder_pad_token_mask)\n",
    "        return input_ids\n",
    "\n",
    "    def forward(self, output_ids: torch.Tensor, decoder_delay_pattern_mask: torch.Tensor, pad_token_id: int):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        batch_size = 1 # We will only allow sampling of single samples for now, otherwise it might be too slow\n",
    "\n",
    "        # apply the pattern mask to the final ids\n",
    "        output_ids = self.apply_delay_pattern_mask(output_ids, decoder_delay_pattern_mask)\n",
    "\n",
    "        # revert the pattern delay mask by filtering the pad token id\n",
    "        output_ids = output_ids[output_ids != pad_token_id].reshape(\n",
    "            batch_size, 8, -1\n",
    "        )\n",
    "\n",
    "        # append the frame dimension back to the audio codes\n",
    "        output_ids = output_ids[None, ...]\n",
    "\n",
    "        audio_scales = [None] * batch_size\n",
    "\n",
    "        codec_outputs_left = self.audio_encoder.decode(output_ids[:, :, ::2, :], audio_scales=audio_scales)\n",
    "        output_values_left = codec_outputs_left.audio_values\n",
    "\n",
    "        codec_outputs_right = self.audio_encoder.decode(output_ids[:, :, 1::2, :], audio_scales=audio_scales)\n",
    "        output_values_right = codec_outputs_right.audio_values\n",
    "\n",
    "        output_values = torch.cat([output_values_left, output_values_right], dim=1)\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_decoder_wrapper = DecodeAudioWrapper(model.audio_encoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'output_ids': {1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "    'decoder_delay_pattern_mask': {1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_output_ids = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "dummy_decoder_delay_pattern_mask = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "dummy_pad_token_id = torch.tensor(2048, dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    audio_decoder_wrapper,\n",
    "    (dummy_output_ids, dummy_decoder_delay_pattern_mask, dummy_pad_token_id),\n",
    "    f\"{folder}/audio_token_decoder.onnx\", \n",
    "    input_names=[\n",
    "        'output_ids',\n",
    "        'decoder_delay_pattern_mask',\n",
    "        'pad_token_id'\n",
    "    ],\n",
    "    output_names=['output_values'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the pre_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLoop(nn.Module):\n",
    "    def __init__(self, num_codebooks=8, audio_channels=2):\n",
    "        super().__init__()\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.audio_channels = audio_channels\n",
    "\n",
    "    @staticmethod\n",
    "    def build_delay_pattern_mask(self, input_ids, pad_token_id, max_length):\n",
    "        input_ids = input_ids.reshape(-1, self.num_codebooks, input_ids.shape[-1])\n",
    "        bsz, num_codebooks, seq_len = input_ids.shape\n",
    "\n",
    "        max_length = max_length.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "        condition = (max_length > 0).long()  # Create a condition tensor\n",
    "\n",
    "        input_ids_shifted = condition * (\n",
    "            torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
    "        )\n",
    "\n",
    "        max_length = max_length.item()\n",
    "\n",
    "        channel_codebooks = num_codebooks // 2 if self.audio_channels == 2 else num_codebooks\n",
    "        # we only apply the mask if we have a large enough seq len - otherwise we return as is\n",
    "        if max_length < 2 * channel_codebooks - 1:\n",
    "            return input_ids.reshape(bsz * num_codebooks, -1), input_ids_shifted.reshape(bsz * num_codebooks, -1)\n",
    "\n",
    "        # fill the shifted ids with the prompt entries, offset by the codebook idx\n",
    "        for codebook in range(channel_codebooks):\n",
    "            if self.audio_channels == 1:\n",
    "                # mono channel - loop over the codebooks one-by-one\n",
    "                input_ids_shifted[:, codebook, codebook : seq_len + codebook] = input_ids[:, codebook]\n",
    "            else:\n",
    "                # left/right channels are interleaved in the generated codebooks, so handle one then the other\n",
    "                input_ids_shifted[:, 2 * codebook, codebook : seq_len + codebook] = input_ids[:, 2 * codebook]\n",
    "                input_ids_shifted[:, 2 * codebook + 1, codebook : seq_len + codebook] = input_ids[:, 2 * codebook + 1]\n",
    "        # construct a pattern mask that indicates the positions of padding tokens for each codebook\n",
    "        # first fill the upper triangular part (the EOS padding)\n",
    "        delay_pattern = torch.triu(\n",
    "            torch.ones((channel_codebooks, max_length), dtype=torch.bool), diagonal=max_length - channel_codebooks + 1\n",
    "        ).to(torch.int64)\n",
    "        # then fill the lower triangular part (the BOS padding)\n",
    "        delay_pattern = delay_pattern + torch.tril(torch.ones((channel_codebooks, max_length), dtype=torch.int64))\n",
    "\n",
    "        if self.audio_channels == 2:\n",
    "            # for left/right channel we need to duplicate every row of the pattern mask in an interleaved fashion\n",
    "            delay_pattern = delay_pattern.repeat_interleave(2, dim=0)\n",
    "\n",
    "        delay_pattern = delay_pattern.to(torch.bool)\n",
    "\n",
    "        mask = ~delay_pattern.to(input_ids.device)\n",
    "        input_ids = mask * input_ids_shifted + ~mask * pad_token_id\n",
    "\n",
    "        # find the first position to start generating - this is the first place we have the -1 token\n",
    "        # and will always be in the first codebook (since it has no codebook offset)\n",
    "        first_codebook_ids = input_ids[:, 0, :]\n",
    "        start_ids = (first_codebook_ids == -1).nonzero()[:, 1]\n",
    "        if len(start_ids) > 0:\n",
    "            first_start_id = min(start_ids)\n",
    "        else:\n",
    "            # we have no tokens that need to be filled - return entire matrix of input ids\n",
    "            first_start_id = seq_len\n",
    "\n",
    "        # (bsz * num_codebooks, seq_len) -> (bsz, num_codebooks, seq_len)\n",
    "        pattern_mask = input_ids.reshape(bsz * num_codebooks, -1)\n",
    "        input_ids = input_ids[..., :first_start_id].reshape(bsz * num_codebooks, -1)\n",
    "        \n",
    "        return input_ids, pattern_mask \n",
    "\n",
    "    def forward(self, batch_size: torch.Tensor, decoder_input_ids=None, decoder_attention_mask=None, max_length=torch.tensor(256, dtype=torch.int64)):\n",
    "        # Equal to #5 _prepare_decoder_input_ids_for_generation\n",
    "        decoder_start_token_id = 2048\n",
    "        pad_token_id = 2048\n",
    "        decoder_input_ids_start = (\n",
    "            torch.ones((batch_size * self.num_codebooks, 1), dtype=torch.long) * decoder_start_token_id\n",
    "        )\n",
    "\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_input_ids_start\n",
    "        elif (decoder_input_ids[..., 0] != decoder_start_token_id).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids_start, decoder_input_ids], dim=-1)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "\n",
    "        # Build delay pattern mask\n",
    "        decoder_input_ids, decoder_delay_pattern_mask = self.build_delay_pattern_mask(self=self, input_ids=decoder_input_ids, pad_token_id=pad_token_id, max_length=max_length)\n",
    "        \n",
    "        return decoder_input_ids, decoder_delay_pattern_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_loop = PreLoop(model.config.decoder.num_codebooks, model.config.decoder.audio_channels)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    # 'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    # 'decoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'decoder_delay_pattern_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_batch_size = torch.tensor(1, dtype=torch.int64)\n",
    "dummy_max_length = torch.tensor(256, dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    pre_loop,\n",
    "    (dummy_batch_size, None, None, dummy_max_length),\n",
    "    f\"{folder}/pre_loop.onnx\",\n",
    "    input_names=['batch_size', 'max_length'],\n",
    "    output_names=['decoder_input_ids', 'decoder_delay_pattern_mask'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the inner_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then export this version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/audio_token_decoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = output_ids.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = decoder_delay_pattern_mask.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "pad_token_np = torch.tensor(2048, dtype=torch.int64).detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    # 'input_ids': np.expand_dims(np.concatenate((input_ids_np, attention_mask_np), axis=0), 0),\n",
    "    'output_ids': input_ids_np,\n",
    "    'decoder_delay_pattern_mask': attention_mask_np,\n",
    "    'pad_token_id': pad_token_np\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.text_encoder.encoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.quantizer.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder._decode_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the audio_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/text_encoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = inputs['input_ids'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = inputs['attention_mask'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'input_ids': input_ids_np,\n",
    "    'attention_mask': attention_mask_np,\n",
    "    'cfg': np.array([3], dtype=np.int64)\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]\n",
    "encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
