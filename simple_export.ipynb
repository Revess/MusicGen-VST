{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch, os, glob, json, math\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import inspect\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "\n",
    "folder = './musicgen-stereo-small'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth asd asd asd\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "raise\n",
    "\n",
    "res = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=259, temperature=2.0, top_k=500, top_p=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model\n",
    "TODO:\n",
    "- [x] Configuration lists exported\n",
    "- [x] Text encoder exported\n",
    "- [x] Projection layer exported\n",
    "- [x] Decoder layer exported\n",
    "- [ ] Sampling function exported\n",
    "- [x] Output decoder exported\n",
    "- [ ] Look at making layers efficient\n",
    "- [ ] Full model throughput test\n",
    "- [ ] Research way to export the sample input version\n",
    "\n",
    "Flow:                                                           This will be in a forloop\n",
    "tokenized inputs and mask -> Text Encoder -> PreLoop -> [Sample] -> Audio Encoder -> Wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.save_pretrained(f'{folder}')\n",
    "processor.save_pretrained(f'{folder}')\n",
    "model.config.to_json_file(f'{folder}/config.json')\n",
    "model.generation_config.to_json_file(f'{folder}/generation_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderWrapper(nn.Module):\n",
    "    def __init__(self, text_encoder):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cfg=None):\n",
    "        last_hidden_state = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        if cfg is not None:\n",
    "            cfg_tensor = cfg.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "            condition = (cfg_tensor > 1).float()  # Create a condition tensor\n",
    "            if condition: # This enforces the addition of cfg as a variable\n",
    "                last_hidden_state = condition * torch.concatenate([last_hidden_state, torch.zeros_like(last_hidden_state)], dim=0)\n",
    "                res_attention_mask = condition * torch.concatenate([attention_mask, torch.zeros_like(attention_mask)], dim=0)\n",
    "        else:\n",
    "            res_attention_mask = attention_mask\n",
    "\n",
    "        return last_hidden_state, res_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/utils.py:2078: UserWarning: Provided key encoded for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_19692/1094539474.py:12: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if condition: # This enforces the addition of cfg as a variable\n"
     ]
    }
   ],
   "source": [
    "text_encoder_wrapper = TextEncoderWrapper(model.text_encoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoded': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_input_ids = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "dummy_attention_mask = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "dummy_cfg = torch.tensor(3, dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    text_encoder_wrapper,\n",
    "    (dummy_input_ids, dummy_attention_mask, dummy_cfg),\n",
    "    f\"{folder}/text_encoder.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask', 'cfg'],\n",
    "    output_names=['last_hidden_state', 'res_attention_mask'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the projection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.past_key_values = None\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states, encoder_attention_mask):\n",
    "        outputs = self.decoder(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = None,\n",
    "            encoder_hidden_states = encoder_hidden_states,\n",
    "            encoder_attention_mask = encoder_attention_mask,\n",
    "            head_mask = None,\n",
    "            cross_attn_head_mask = None,\n",
    "            past_key_values = self.past_key_values,\n",
    "            inputs_embeds = None,\n",
    "            labels = None,\n",
    "            use_cache = True,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            return_dict = True\n",
    "        )\n",
    "\n",
    "        self.past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_wrapper = DecoderWrapper(model.decoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size'},\n",
    "    'encoder_hidden_states': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'encoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_input_ids = torch.randint(0, 100, (16, 1), dtype=torch.int64)\n",
    "dummy_encoder_hidden_states = torch.randn((2, 18, 1024), dtype=torch.float32)\n",
    "dummy_encoder_attention_mask = torch.randint(0, 100, (2, 18), dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    decoder_wrapper,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_encoder_hidden_states,\n",
    "        dummy_encoder_attention_mask,\n",
    "    ),\n",
    "    f\"{folder}/decoder.onnx\",\n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'encoder_hidden_states',\n",
    "        'encoder_attention_mask',\n",
    "    ],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the audio_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a PT wrapper for the decoding portion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeAudioWrapper(nn.Module):\n",
    "    def __init__(self, audio_encoder):\n",
    "        super().__init__()\n",
    "        self.audio_encoder = audio_encoder\n",
    "\n",
    "    def apply_delay_pattern_mask(self, input_ids, decoder_pad_token_mask):\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        decoder_pad_token_mask = decoder_pad_token_mask[..., :seq_len]\n",
    "        input_ids = torch.where(decoder_pad_token_mask == -1, input_ids, decoder_pad_token_mask)\n",
    "        return input_ids\n",
    "\n",
    "    def forward(self, output_ids: torch.Tensor, decoder_delay_pattern_mask: torch.Tensor, pad_token_id: int):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        batch_size = 1 # We will only allow sampling of single samples for now, otherwise it might be too slow\n",
    "\n",
    "        # apply the pattern mask to the final ids\n",
    "        output_ids = self.apply_delay_pattern_mask(output_ids, decoder_delay_pattern_mask)\n",
    "\n",
    "        # revert the pattern delay mask by filtering the pad token id\n",
    "        output_ids = output_ids[output_ids != pad_token_id].reshape(\n",
    "            batch_size, 8, -1\n",
    "        )\n",
    "\n",
    "        # append the frame dimension back to the audio codes\n",
    "        output_ids = output_ids[None, ...]\n",
    "\n",
    "        audio_scales = [None] * batch_size\n",
    "\n",
    "        codec_outputs_left = self.audio_encoder.decode(output_ids[:, :, ::2, :], audio_scales=audio_scales)\n",
    "        output_values_left = codec_outputs_left.audio_values\n",
    "\n",
    "        codec_outputs_right = self.audio_encoder.decode(output_ids[:, :, 1::2, :], audio_scales=audio_scales)\n",
    "        output_values_right = codec_outputs_right.audio_values\n",
    "\n",
    "        output_values = torch.cat([output_values_left, output_values_right], dim=1)\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:736: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(audio_codes) != 1:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:433: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  quantized_out = torch.tensor(0.0, device=codes.device)\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:434: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, indices in enumerate(codes):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:144: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_pad = max(padding_left, padding_right)\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if length <= max_pad:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "audio_decoder_wrapper = DecodeAudioWrapper(model.audio_encoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'output_ids': {1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "    'decoder_delay_pattern_mask': {1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_output_ids = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "dummy_decoder_delay_pattern_mask = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "dummy_pad_token_id = torch.tensor([2048], dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    audio_decoder_wrapper,\n",
    "    (dummy_output_ids, dummy_decoder_delay_pattern_mask, dummy_pad_token_id),\n",
    "    f\"{folder}/audio_token_decoder.onnx\", \n",
    "    input_names=[\n",
    "        'output_ids',\n",
    "        'decoder_delay_pattern_mask',\n",
    "        'pad_token_id'\n",
    "    ],\n",
    "    output_names=['output_values'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the pre_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLoop(nn.Module):\n",
    "    def __init__(self, num_codebooks=8, audio_channels=2):\n",
    "        super().__init__()\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.audio_channels = audio_channels\n",
    "\n",
    "    @staticmethod\n",
    "    def build_delay_pattern_mask(self, input_ids, pad_token_id, max_length):\n",
    "        input_ids = input_ids.reshape(-1, self.num_codebooks, input_ids.shape[-1])\n",
    "        bsz, num_codebooks, seq_len = input_ids.shape\n",
    "\n",
    "        max_length = max_length.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "        condition = (max_length > 0).long()  # Create a condition tensor\n",
    "\n",
    "        input_ids_shifted = condition * (\n",
    "            torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
    "        )\n",
    "\n",
    "        max_length = max_length.item()\n",
    "\n",
    "        channel_codebooks = num_codebooks // 2 if self.audio_channels == 2 else num_codebooks\n",
    "        # we only apply the mask if we have a large enough seq len - otherwise we return as is\n",
    "        if max_length < 2 * channel_codebooks - 1:\n",
    "            return input_ids.reshape(bsz * num_codebooks, -1), input_ids_shifted.reshape(bsz * num_codebooks, -1)\n",
    "\n",
    "        # fill the shifted ids with the prompt entries, offset by the codebook idx\n",
    "        for codebook in range(channel_codebooks):\n",
    "            if self.audio_channels == 1:\n",
    "                # mono channel - loop over the codebooks one-by-one\n",
    "                input_ids_shifted[:, codebook, codebook : seq_len + codebook] = input_ids[:, codebook]\n",
    "            else:\n",
    "                # left/right channels are interleaved in the generated codebooks, so handle one then the other\n",
    "                input_ids_shifted[:, 2 * codebook, codebook : seq_len + codebook] = input_ids[:, 2 * codebook]\n",
    "                input_ids_shifted[:, 2 * codebook + 1, codebook : seq_len + codebook] = input_ids[:, 2 * codebook + 1]\n",
    "        # construct a pattern mask that indicates the positions of padding tokens for each codebook\n",
    "        # first fill the upper triangular part (the EOS padding)\n",
    "        delay_pattern = torch.triu(\n",
    "            torch.ones((channel_codebooks, max_length), dtype=torch.bool), diagonal=max_length - channel_codebooks + 1\n",
    "        ).to(torch.int64)\n",
    "        # then fill the lower triangular part (the BOS padding)\n",
    "        delay_pattern = delay_pattern + torch.tril(torch.ones((channel_codebooks, max_length), dtype=torch.int64))\n",
    "\n",
    "        if self.audio_channels == 2:\n",
    "            # for left/right channel we need to duplicate every row of the pattern mask in an interleaved fashion\n",
    "            delay_pattern = delay_pattern.repeat_interleave(2, dim=0)\n",
    "\n",
    "        delay_pattern = delay_pattern.to(torch.bool)\n",
    "\n",
    "        mask = ~delay_pattern.to(input_ids.device)\n",
    "        input_ids = mask * input_ids_shifted + ~mask * pad_token_id\n",
    "\n",
    "        # find the first position to start generating - this is the first place we have the -1 token\n",
    "        # and will always be in the first codebook (since it has no codebook offset)\n",
    "        first_codebook_ids = input_ids[:, 0, :]\n",
    "        start_ids = (first_codebook_ids == -1).nonzero()[:, 1]\n",
    "        if len(start_ids) > 0:\n",
    "            first_start_id = min(start_ids)\n",
    "        else:\n",
    "            # we have no tokens that need to be filled - return entire matrix of input ids\n",
    "            first_start_id = seq_len\n",
    "\n",
    "        # (bsz * num_codebooks, seq_len) -> (bsz, num_codebooks, seq_len)\n",
    "        pattern_mask = input_ids.reshape(bsz * num_codebooks, -1)\n",
    "        input_ids = input_ids[..., :first_start_id].reshape(bsz * num_codebooks, -1)\n",
    "        \n",
    "        return input_ids, pattern_mask \n",
    "\n",
    "    def forward(self, batch_size: torch.Tensor, decoder_input_ids=None, decoder_attention_mask=None, max_length=torch.tensor(256, dtype=torch.int64)):\n",
    "        # TODO: Impl for audio input (uses decoder_input_ids and decoder_attention_mask)\n",
    "        # Equal to #5 _prepare_decoder_input_ids_for_generation\n",
    "        decoder_start_token_id = 2048\n",
    "        pad_token_id = 2048\n",
    "        decoder_input_ids_start = (\n",
    "            torch.ones((batch_size * self.num_codebooks, 1), dtype=torch.long) * decoder_start_token_id\n",
    "        )\n",
    "\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_input_ids_start\n",
    "        elif (decoder_input_ids[..., 0] != decoder_start_token_id).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids_start, decoder_input_ids], dim=-1)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "\n",
    "        # Build delay pattern mask\n",
    "        decoder_input_ids, decoder_delay_pattern_mask = self.build_delay_pattern_mask(self=self, input_ids=decoder_input_ids, pad_token_id=pad_token_id, max_length=max_length)\n",
    "        \n",
    "        return decoder_input_ids, decoder_delay_pattern_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19692/1985547935.py:16: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
      "/tmp/ipykernel_19692/1985547935.py:19: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_length = max_length.item()\n",
      "/tmp/ipykernel_19692/1985547935.py:23: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_length < 2 * channel_codebooks - 1:\n",
      "/tmp/ipykernel_19692/1985547935.py:56: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(start_ids) > 0:\n",
      "/tmp/ipykernel_19692/1985547935.py:57: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  first_start_id = min(start_ids)\n",
      "/tmp/ipykernel_19692/1985547935.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  first_start_id = min(start_ids)\n"
     ]
    }
   ],
   "source": [
    "pre_loop = PreLoop(model.config.decoder.num_codebooks, model.config.decoder.audio_channels)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    # 'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    # 'decoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'decoder_delay_pattern_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_batch_size = torch.tensor(1, dtype=torch.int64)\n",
    "dummy_max_length = torch.tensor(256, dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    pre_loop,\n",
    "    (dummy_batch_size, None, None, dummy_max_length),\n",
    "    f\"{folder}/pre_loop.onnx\",\n",
    "    input_names=['batch_size', 'max_length'],\n",
    "    output_names=['decoder_input_ids', 'decoder_delay_pattern_mask'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(nn.Module):\n",
    "    def __init__(self, decoder, enc_proj):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.enc_proj = enc_proj\n",
    "        self.past_key_values = None\n",
    "        self.filter_value = -float('inf')\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_delay_pattern_mask(input_ids, decoder_pad_token_mask):\n",
    "        \"\"\"Apply a delay pattern mask to the decoder input ids, only preserving predictions where\n",
    "        the mask is set to -1, and otherwise setting to the value detailed in the mask.\"\"\"\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        decoder_pad_token_mask = decoder_pad_token_mask[..., :seq_len]\n",
    "        input_ids = torch.where(decoder_pad_token_mask == -1, input_ids, decoder_pad_token_mask)\n",
    "        return input_ids\n",
    "    \n",
    "    def logits_process(self, next_token_logits, cfg):\n",
    "        # ClassifierFreeGuidanceLogitsProcessor\n",
    "        unguided_bsz = next_token_logits.shape[0] // 2\n",
    "        cond_logits, uncond_logits = next_token_logits.split(unguided_bsz, dim=0)\n",
    "        next_token_scores = uncond_logits + (cond_logits - uncond_logits) * cfg\n",
    "        return next_token_scores\n",
    "\n",
    "    def logits_warp(self, scores: torch.Tensor, temperature: torch.Tensor, topk: torch.Tensor, topp: torch.Tensor):\n",
    "        # Temperature\n",
    "        scores_processed = scores / temperature\n",
    "\n",
    "        # Topk\n",
    "        top_k = min(topk, scores_processed.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores_processed < torch.topk(scores_processed, top_k)[0][..., -1, None]\n",
    "        scores_processed = scores_processed.masked_fill(indices_to_remove, self.filter_value)\n",
    "\n",
    "        # Topp\n",
    "        sorted_logits, sorted_indices = torch.sort(scores_processed, descending=False)\n",
    "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs <= (1 - topp)\n",
    "        # Keep at least min_tokens_to_keep\n",
    "        sorted_indices_to_remove[..., -1 :] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        scores_processed = scores_processed.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores_processed\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            decoder_input_ids, \n",
    "            attention_mask, \n",
    "            encoder_hidden_states, \n",
    "            delay_pattern_mask, \n",
    "            cfg = torch.tensor(3),\n",
    "            temperature = torch.tensor(0.7), \n",
    "            topk = torch.tensor(500), \n",
    "            topp = torch.tensor(0.0)\n",
    "        ):\n",
    "\n",
    "        # Input prep\n",
    "        model_inputs = self.apply_delay_pattern_mask(decoder_input_ids, delay_pattern_mask)\n",
    "\n",
    "        if cfg is not None:\n",
    "            cfg_tensor = cfg.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "            condition = (cfg_tensor > 1).int()  # Create a condition tensor\n",
    "            if condition:\n",
    "                model_inputs = condition * model_inputs.repeat((2,1))\n",
    "                if attention_mask is not None:\n",
    "                    model_input_attention_mask = condition * attention_mask.repeat((2,1))\n",
    "        \n",
    "        if self.past_key_values is not None:\n",
    "            model_inputs = model_inputs[:, -1:]\n",
    "\n",
    "        # Forward Loop\n",
    "        encoder_hidden_states = self.enc_proj(encoder_hidden_states)\n",
    "        encoder_hidden_states = encoder_hidden_states * model_input_attention_mask[..., None]\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            input_ids = model_inputs,\n",
    "            attention_mask = None,\n",
    "            encoder_hidden_states = encoder_hidden_states,\n",
    "            encoder_attention_mask = model_input_attention_mask,\n",
    "            head_mask = None,\n",
    "            cross_attn_head_mask = None,\n",
    "            past_key_values = self.past_key_values,\n",
    "            inputs_embeds = None,\n",
    "            labels = None,\n",
    "            use_cache = True,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            return_dict = True\n",
    "        )\n",
    "\n",
    "        self.past_key_values = outputs.past_key_values\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :].clone()\n",
    "\n",
    "        # CFG processing if cfg is large enough, aka logits_processlist\n",
    "        if condition:\n",
    "            next_token_scores = self.logits_process(next_token_logits, cfg)\n",
    "        else:\n",
    "            next_token_scores = next_token_logits\n",
    "\n",
    "        next_token_scores = self.logits_warp(next_token_scores, temperature, topk, topp)\n",
    "\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "        return decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19692/3430601900.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if condition:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.weights.size(0):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:296: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:264: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/tmp/ipykernel_19692/3430601900.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if condition:\n",
      "/tmp/ipykernel_19692/3430601900.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  top_k = min(topk, scores_processed.size(-1))  # Safety check\n"
     ]
    }
   ],
   "source": [
    "sample = Sample(model.decoder, model.enc_to_dec_proj)\n",
    "sample.eval()\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'encoder_hidden_states': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'delay_pattern_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_decoder_input_ids = torch.randint(0, 100, (8, 1), dtype=torch.int64)\n",
    "dummy_attention_mask = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "dummy_encoder_hidden_states = torch.randn((2, 18, 768), dtype=torch.float32)\n",
    "dummy_delay_pattern_mask = torch.randint(0, 100, (8, 260), dtype=torch.int64)\n",
    "dummy_cfg = torch.tensor(3, dtype=torch.int64)\n",
    "dummy_temperature = torch.tensor(0.7, dtype=torch.float32)\n",
    "dummy_topk = torch.tensor(500, dtype=torch.int64)\n",
    "dummy_topp = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    sample,\n",
    "    (\n",
    "        dummy_decoder_input_ids, \n",
    "        dummy_attention_mask, \n",
    "        dummy_encoder_hidden_states, \n",
    "        dummy_delay_pattern_mask, \n",
    "        dummy_cfg, \n",
    "        dummy_temperature, \n",
    "        dummy_topk, \n",
    "        dummy_topp\n",
    "    ),\n",
    "    f\"{folder}/sampler.onnx\",\n",
    "    input_names=[\n",
    "        'decoder_input_ids', \n",
    "        'attention_mask', \n",
    "        'encoder_hidden_states', \n",
    "        'delay_pattern_mask', \n",
    "        'cfg', \n",
    "        'temperature', \n",
    "        'topk', \n",
    "        'topp'\n",
    "    ],\n",
    "    output_names=['decoder_input_ids'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/text_encoder.onnx\")\n",
    "\n",
    "input_ids_np = inputs['input_ids'].detach().numpy()\n",
    "attention_mask_np = inputs['attention_mask'].detach().numpy()\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'input_ids': input_ids_np,\n",
    "    'attention_mask': attention_mask_np,\n",
    "    'cfg': np.array([3], dtype=np.int64)\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/pre_loop.onnx\")\n",
    "\n",
    "dummy_batch_size = torch.tensor(1, dtype=torch.int64).detach().numpy()\n",
    "dummy_max_length = torch.tensor(256, dtype=torch.int64).detach().numpy()\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'batch_size': dummy_batch_size,\n",
    "    'max_length': dummy_max_length\n",
    "}\n",
    "decoder_input_ids, decoder_delay_pattern_mask = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/sampler.onnx\")\n",
    "\n",
    "for i in range(256):\n",
    "    # Run the model\n",
    "    ort_inputs = {\n",
    "        'decoder_input_ids.1': decoder_input_ids, \n",
    "        'attention_mask': inputs['attention_mask'].detach().numpy(), \n",
    "        'encoder_hidden_states': encoded, \n",
    "        'delay_pattern_mask': decoder_delay_pattern_mask, \n",
    "        'cfg': np.array([3], dtype=np.int64), \n",
    "        'temperature': np.array([0.7], dtype=np.float32), \n",
    "        'topk': np.array([500], dtype=np.int64), \n",
    "        'topp': np.array([0.0], dtype=np.float32)\n",
    "    }\n",
    "\n",
    "    decoder_input_ids = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/audio_token_decoder.onnx\")\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'output_ids': decoder_input_ids[:, :-1], # We either need to remove the first tokens or add tokens to the decoder delay_pattern mask, check og for inspo\n",
    "    'decoder_delay_pattern_mask': decoder_delay_pattern_mask,\n",
    "    'pad_token_id': np.array([2048], dtype=np.int64)\n",
    "}\n",
    "\n",
    "output_values = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "Audio(output_values[0], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=output_values[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and export the full model\n",
    "\n",
    "Doesnt work bc of problems in attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenWrapper(nn.Module):\n",
    "    def __init__(self, model: MusicgenForConditionalGeneration):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, guidance_scale=3, max_new_tokens=256, temperature=2.0, top_k=500, top_p=0.0):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        output_values = model.generate(**inputs, guidance_scale=guidance_scale.item(), max_new_tokens=max_new_tokens.item(), temperature=temperature.item(), top_k=top_k.item(), top_p=top_p.item())\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musicgen_wrapper = MusicGenWrapper(model)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_guidance_scale = torch.tensor(3, dtype=torch.int64)\n",
    "dummy_max_new_tokens = torch.tensor(256, dtype=torch.int64)\n",
    "dummy_temperature = torch.tensor(2.0, dtype=torch.float32)\n",
    "dummy_top_k = torch.tensor(500, dtype=torch.int64)\n",
    "dummy_top_p = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    musicgen_wrapper,\n",
    "    (inputs['input_ids'], inputs['attention_mask'], dummy_guidance_scale, dummy_max_new_tokens, dummy_temperature, dummy_top_k, dummy_top_p),\n",
    "    f\"{folder}/musicgen.onnx\", \n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'attention_mask',\n",
    "        'guidance_scale',\n",
    "        'max_new_tokens',\n",
    "        'temperature',\n",
    "        'top_k',\n",
    "        'top_p'\n",
    "    ],\n",
    "    output_names=['output_values'],\n",
    "    dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/audio_token_decoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = output_ids.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = decoder_delay_pattern_mask.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "pad_token_np = torch.tensor(2048, dtype=torch.int64).detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    # 'input_ids': np.expand_dims(np.concatenate((input_ids_np, attention_mask_np), axis=0), 0),\n",
    "    'output_ids': input_ids_np,\n",
    "    'decoder_delay_pattern_mask': attention_mask_np,\n",
    "    'pad_token_id': pad_token_np\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = f\"{folder}/sampler.onnx\"  # Update this with your ONNX model path\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Print model input names and their shapes\n",
    "print(\"Model Inputs:\")\n",
    "for input_tensor in onnx_model.graph.input:\n",
    "    print(f\"Input name: {input_tensor.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.text_encoder.encoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.quantizer.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder._decode_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the audio_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/text_encoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = inputs['input_ids'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = inputs['attention_mask'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'input_ids': input_ids_np,\n",
    "    'attention_mask': attention_mask_np,\n",
    "    'cfg': np.array([3], dtype=np.int64)\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]\n",
    "encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
