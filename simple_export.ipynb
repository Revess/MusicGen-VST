{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/bas/anaconda3/envs/musiclm/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch, os, glob, json, math\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import inspect\n",
    "\n",
    "export_onnx = False\n",
    "test_onnx = False\n",
    "export_torch_script = False\n",
    "test_torch_script = True\n",
    "do_model_sample = False\n",
    "\n",
    "cfg = 5\n",
    "temperature = 0.7\n",
    "top_k = 500\n",
    "top_p = 0.0\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-stereo-small\")\n",
    "model.eval()\n",
    "\n",
    "folder = './musicgen-stereo-small'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth asd asd asd\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "if do_model_sample:\n",
    "    res = model.generate(**inputs, do_sample=True, guidance_scale=cfg, max_new_tokens=256, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "# res, output_ids = model.generate(**inputs, do_sample=False, guidance_scale=5, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_model_sample:\n",
    "    from IPython.display import Audio\n",
    "\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    Audio(res[0].detach().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model\n",
    "TODO:\n",
    "- [x] Configuration lists exported\n",
    "- [x] Text encoder exported\n",
    "- [x] Projection layer exported\n",
    "- [x] Decoder layer exported\n",
    "- [ ] Sampling function exported\n",
    "- [x] Output decoder exported\n",
    "- [ ] Look at making layers efficient\n",
    "- [ ] Full model throughput test\n",
    "- [ ] Research way to export the sample input version\n",
    "\n",
    "Flow:                                                           This will be in a forloop\n",
    "tokenized inputs and mask -> Text Encoder -> PreLoop -> [Sample] -> Audio Encoder -> Wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.save_pretrained(f'{folder}')\n",
    "processor.save_pretrained(f'{folder}')\n",
    "model.config.to_json_file(f'{folder}/config.json')\n",
    "model.generation_config.to_json_file(f'{folder}/generation_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderWrapper(nn.Module):\n",
    "    def __init__(self, text_encoder):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cfg=None):\n",
    "        last_hidden_state = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        if cfg is not None:\n",
    "            cfg_tensor = cfg.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "            condition = (cfg_tensor > 1).float()  # Create a condition tensor\n",
    "            if condition: # This enforces the addition of cfg as a variable\n",
    "                last_hidden_state = torch.concatenate([condition * last_hidden_state, torch.zeros_like(last_hidden_state)], dim=0)\n",
    "                res_attention_mask = torch.concatenate([condition * attention_mask, torch.zeros_like(attention_mask)], dim=0)\n",
    "        else:\n",
    "            res_attention_mask = attention_mask\n",
    "\n",
    "        return last_hidden_state, res_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_torch_script:\n",
    "    text_encoder_wrapper = TextEncoderWrapper(model.text_encoder)\n",
    "    text_encoder_wrapper.eval()\n",
    "    scripted_model = torch.jit.script(text_encoder_wrapper)  # or torch.jit.trace for simple models\n",
    "    scripted_model.save(f\"{folder}/text_encoder_wrapper.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_onnx:\n",
    "    text_encoder_wrapper = TextEncoderWrapper(model.text_encoder)\n",
    "    text_encoder_wrapper.eval()\n",
    "\n",
    "    # Define the dynamic axes for variable-length input shapes\n",
    "    dynamic_axes = {\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "        'encoded': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "    }\n",
    "\n",
    "    # Example input shapes (with batch size = 2, sequence length = 10)\n",
    "    dummy_input_ids = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "    dummy_attention_mask = torch.randint(0, 100, (1, 18), dtype=torch.int64)\n",
    "    dummy_cfg = torch.tensor(3, dtype=torch.int64)\n",
    "\n",
    "    # Export the model to ONNX format\n",
    "    torch.onnx.export(\n",
    "        text_encoder_wrapper,\n",
    "        (dummy_input_ids, dummy_attention_mask, dummy_cfg),\n",
    "        f\"{folder}/text_encoder.onnx\",\n",
    "        input_names=['input_ids', 'attention_mask', 'cfg'],\n",
    "        output_names=['last_hidden_state', 'res_attention_mask'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=17\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the audio_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a PT wrapper for the decoding portion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeAudioWrapper(nn.Module):\n",
    "    def __init__(self, audio_encoder):\n",
    "        super().__init__()\n",
    "        self.audio_encoder = audio_encoder\n",
    "\n",
    "    def apply_delay_pattern_mask(self, input_ids, decoder_pad_token_mask):\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        decoder_pad_token_mask = decoder_pad_token_mask[..., :seq_len]\n",
    "        input_ids = torch.where(decoder_pad_token_mask == -1, input_ids, decoder_pad_token_mask)\n",
    "        return input_ids\n",
    "\n",
    "    def forward(self, output_ids: torch.Tensor, decoder_delay_pattern_mask: torch.Tensor, pad_token_id: int):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        batch_size = 1 # We will only allow sampling of single samples for now, otherwise it might be too slow\n",
    "\n",
    "        # apply the pattern mask to the final ids\n",
    "        output_ids = self.apply_delay_pattern_mask(output_ids, decoder_delay_pattern_mask)\n",
    "\n",
    "        # revert the pattern delay mask by filtering the pad token id\n",
    "        output_ids = output_ids[output_ids != pad_token_id].reshape(\n",
    "            batch_size, 8, -1\n",
    "        )\n",
    "\n",
    "        # append the frame dimension back to the audio codes\n",
    "        output_ids = output_ids[None, ...]\n",
    "\n",
    "        audio_scales = [None] * batch_size\n",
    "\n",
    "        codec_outputs_left = self.audio_encoder.decode(output_ids[:, :, ::2, :], audio_scales=audio_scales)\n",
    "        output_values_left = codec_outputs_left.audio_values\n",
    "\n",
    "        codec_outputs_right = self.audio_encoder.decode(output_ids[:, :, 1::2, :], audio_scales=audio_scales)\n",
    "        output_values_right = codec_outputs_right.audio_values\n",
    "\n",
    "        output_values = torch.cat([output_values_left, output_values_right], dim=1)\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_torch_script:\n",
    "    audio_decoder_wrapper = DecodeAudioWrapper(model.audio_encoder)\n",
    "    audio_decoder_wrapper.eval()\n",
    "    scripted_model = torch.jit.script(audio_decoder_wrapper)  # or torch.jit.trace for simple models\n",
    "    scripted_model.save(f\"{folder}/audio_decoder_wrapper.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_onnx:\n",
    "    audio_decoder_wrapper = DecodeAudioWrapper(model.audio_encoder)\n",
    "\n",
    "    # Define the dynamic axes for variable-length input shapes\n",
    "    dynamic_axes = {\n",
    "        'output_ids': {1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "        'decoder_delay_pattern_mask': {1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "    }\n",
    "\n",
    "    # Example input shapes (with batch size = 2, sequence length = 10)\n",
    "    dummy_output_ids = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "    dummy_decoder_delay_pattern_mask = torch.randint(0, 100, (8, 257), dtype=torch.int64)\n",
    "    dummy_pad_token_id = torch.tensor([2048], dtype=torch.int64)\n",
    "\n",
    "    # Export the model to ONNX format\n",
    "    torch.onnx.export(\n",
    "        audio_decoder_wrapper,\n",
    "        (dummy_output_ids, dummy_decoder_delay_pattern_mask, dummy_pad_token_id),\n",
    "        f\"{folder}/audio_token_decoder.onnx\", \n",
    "        input_names=[\n",
    "            'output_ids',\n",
    "            'decoder_delay_pattern_mask',\n",
    "            'pad_token_id'\n",
    "        ],\n",
    "        output_names=['output_values'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=17\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the pre_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLoop(nn.Module):\n",
    "    def __init__(self, num_codebooks=8, audio_channels=2):\n",
    "        super().__init__()\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.audio_channels = audio_channels\n",
    "\n",
    "    @staticmethod\n",
    "    def build_delay_pattern_mask(self, input_ids, pad_token_id, max_length):\n",
    "        input_ids = input_ids.reshape(-1, self.num_codebooks, input_ids.shape[-1])\n",
    "        bsz, num_codebooks, seq_len = input_ids.shape\n",
    "\n",
    "        max_length = max_length.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "        condition = (max_length > 0).long()  # Create a condition tensor\n",
    "\n",
    "        input_ids_shifted = condition * (\n",
    "            torch.ones((bsz, num_codebooks, max_length.item()), dtype=torch.long, device=input_ids.device) * -1\n",
    "        )\n",
    "\n",
    "        max_length = max_length.item()\n",
    "\n",
    "        channel_codebooks = num_codebooks // 2 if self.audio_channels == 2 else num_codebooks\n",
    "        # we only apply the mask if we have a large enough seq len - otherwise we return as is\n",
    "        if max_length < 2 * channel_codebooks - 1:\n",
    "            return input_ids.reshape(bsz * num_codebooks, -1), input_ids_shifted.reshape(bsz * num_codebooks, -1)\n",
    "\n",
    "        # fill the shifted ids with the prompt entries, offset by the codebook idx\n",
    "        for codebook in range(channel_codebooks):\n",
    "            if self.audio_channels == 1:\n",
    "                # mono channel - loop over the codebooks one-by-one\n",
    "                input_ids_shifted[:, codebook, codebook : seq_len + codebook] = input_ids[:, codebook]\n",
    "            else:\n",
    "                # left/right channels are interleaved in the generated codebooks, so handle one then the other\n",
    "                input_ids_shifted[:, 2 * codebook, codebook : seq_len + codebook] = input_ids[:, 2 * codebook]\n",
    "                input_ids_shifted[:, 2 * codebook + 1, codebook : seq_len + codebook] = input_ids[:, 2 * codebook + 1]\n",
    "        # construct a pattern mask that indicates the positions of padding tokens for each codebook\n",
    "        # first fill the upper triangular part (the EOS padding)\n",
    "        delay_pattern = torch.triu(\n",
    "            torch.ones((channel_codebooks, max_length), dtype=torch.bool), diagonal=max_length - channel_codebooks + 1\n",
    "        ).to(torch.int64)\n",
    "        # then fill the lower triangular part (the BOS padding)\n",
    "        delay_pattern = delay_pattern + torch.tril(torch.ones((channel_codebooks, max_length), dtype=torch.int64))\n",
    "\n",
    "        if self.audio_channels == 2:\n",
    "            # for left/right channel we need to duplicate every row of the pattern mask in an interleaved fashion\n",
    "            delay_pattern = delay_pattern.repeat_interleave(2, dim=0)\n",
    "\n",
    "        delay_pattern = delay_pattern.to(torch.bool)\n",
    "\n",
    "        mask = ~delay_pattern.to(input_ids.device)\n",
    "        input_ids = mask * input_ids_shifted + ~mask * pad_token_id\n",
    "\n",
    "        # find the first position to start generating - this is the first place we have the -1 token\n",
    "        # and will always be in the first codebook (since it has no codebook offset)\n",
    "        first_codebook_ids = input_ids[:, 0, :]\n",
    "        start_ids = (first_codebook_ids == -1).nonzero()[:, 1]\n",
    "        if len(start_ids) > 0:\n",
    "            first_start_id = min(start_ids)\n",
    "        else:\n",
    "            # we have no tokens that need to be filled - return entire matrix of input ids\n",
    "            first_start_id = seq_len\n",
    "\n",
    "        # (bsz * num_codebooks, seq_len) -> (bsz, num_codebooks, seq_len)\n",
    "        pattern_mask = input_ids.reshape(bsz * num_codebooks, -1)\n",
    "        input_ids = input_ids[..., :first_start_id].reshape(bsz * num_codebooks, -1)\n",
    "        \n",
    "        return input_ids, pattern_mask \n",
    "\n",
    "    def forward(self, batch_size: torch.Tensor, decoder_input_ids=None, decoder_attention_mask=None, max_length=torch.tensor(256, dtype=torch.int64)):\n",
    "        # TODO: Impl for audio input (uses decoder_input_ids and decoder_attention_mask)\n",
    "        # Equal to #5 _prepare_decoder_input_ids_for_generation\n",
    "        decoder_start_token_id = 2048\n",
    "        pad_token_id = 2048\n",
    "        decoder_input_ids_start = (\n",
    "            torch.ones((batch_size * self.num_codebooks, 1), dtype=torch.long) * decoder_start_token_id\n",
    "        )\n",
    "\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_input_ids_start\n",
    "        elif (decoder_input_ids[..., 0] != decoder_start_token_id).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids_start, decoder_input_ids], dim=-1)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                decoder_attention_mask = decoder_attention_mask\n",
    "\n",
    "        # Build delay pattern mask\n",
    "        decoder_input_ids, decoder_delay_pattern_mask = self.build_delay_pattern_mask(self=self, input_ids=decoder_input_ids, pad_token_id=pad_token_id, max_length=max_length)\n",
    "        \n",
    "        return decoder_input_ids, decoder_delay_pattern_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_torch_script:\n",
    "    pre_loop = PreLoop(model.config.decoder.num_codebooks, model.config.decoder.audio_channels)\n",
    "    pre_loop.eval()\n",
    "    scripted_model = torch.jit.script(pre_loop)  # or torch.jit.trace for simple models\n",
    "    scripted_model.save(f\"{folder}/pre_loop.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_onnx:\n",
    "    pre_loop = PreLoop(model.config.decoder.num_codebooks, model.config.decoder.audio_channels)\n",
    "\n",
    "    # Define the dynamic axes for variable-length input shapes\n",
    "    dynamic_axes = {\n",
    "        # 'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        # 'decoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'decoder_delay_pattern_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "    }\n",
    "\n",
    "    # Example input shapes (with batch size = 2, sequence length = 10)\n",
    "    dummy_batch_size = torch.tensor(1, dtype=torch.int64)\n",
    "    dummy_max_length = torch.tensor(256, dtype=torch.int64)\n",
    "\n",
    "    # Export the model to ONNX format\n",
    "    torch.onnx.export(\n",
    "        pre_loop,\n",
    "        (dummy_batch_size, None, None, dummy_max_length),\n",
    "        f\"{folder}/pre_loop.onnx\",\n",
    "        input_names=['batch_size', 'max_length'],\n",
    "        output_names=['decoder_input_ids', 'decoder_delay_pattern_mask'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=17\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(nn.Module):\n",
    "    def __init__(self, decoder, enc_proj):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.enc_proj = enc_proj\n",
    "        self.filter_value = -float('inf')\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_delay_pattern_mask(input_ids, decoder_pad_token_mask):\n",
    "        \"\"\"Apply a delay pattern mask to the decoder input ids, only preserving predictions where\n",
    "        the mask is set to -1, and otherwise setting to the value detailed in the mask.\"\"\"\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        decoder_pad_token_mask = decoder_pad_token_mask[..., :seq_len]\n",
    "        input_ids = torch.where(decoder_pad_token_mask == -1, input_ids, decoder_pad_token_mask)\n",
    "        return input_ids\n",
    "    \n",
    "    def logits_process(self, next_token_logits, cfg):\n",
    "        # ClassifierFreeGuidanceLogitsProcessor\n",
    "        unguided_bsz = next_token_logits.shape[0] // 2\n",
    "        cond_logits, uncond_logits = next_token_logits.split(unguided_bsz, dim=0)\n",
    "        next_token_scores = uncond_logits + (cond_logits - uncond_logits) * cfg\n",
    "        return next_token_scores\n",
    "\n",
    "    def logits_warp(self, scores: torch.Tensor, temperature: torch.Tensor, topk: torch.Tensor, topp: torch.Tensor):\n",
    "        # Temperature\n",
    "        if temperature != 1.0:\n",
    "            scores_processed = scores / temperature\n",
    "        else:\n",
    "            scores_processed = scores\n",
    "\n",
    "        # Topk\n",
    "        top_k = min(topk, scores_processed.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores_processed < torch.topk(scores_processed, top_k)[0][..., -1, None]\n",
    "        scores_processed = scores_processed.masked_fill(indices_to_remove, self.filter_value)\n",
    "\n",
    "        # Topp\n",
    "        if topp < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(scores_processed, descending=False)\n",
    "            cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
    "            sorted_indices_to_remove = cumulative_probs <= (1 - topp)\n",
    "            # Keep at least min_tokens_to_keep\n",
    "            sorted_indices_to_remove[..., -1 :] = 0\n",
    "\n",
    "            # scatter sorted tensors to original indexing\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            scores_processed = scores_processed.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores_processed\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            decoder_input_ids, \n",
    "            attention_mask, # <- Is encoder attention_mask\n",
    "            encoder_hidden_states, \n",
    "            delay_pattern_mask, \n",
    "            cfg = torch.tensor(3),\n",
    "            temperature = torch.tensor(0.7), \n",
    "            topk = torch.tensor(500), \n",
    "            topp = torch.tensor(0.0),\n",
    "            past_key_values_a = torch.tensor([-1]),\n",
    "            past_key_values_b = torch.tensor([-1])\n",
    "        ):\n",
    "\n",
    "        if past_key_values_a[0] == -1 and past_key_values_b[0] == -1:\n",
    "            past_key_values = None\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Input prep\n",
    "        model_inputs = self.apply_delay_pattern_mask(decoder_input_ids, delay_pattern_mask)\n",
    "\n",
    "        if cfg is not None:\n",
    "            cfg_tensor = cfg.unsqueeze(0)  # Convert to tensor for ONNX\n",
    "            condition = (cfg_tensor > 1).int()  # Create a condition tensor\n",
    "            if condition:\n",
    "                model_inputs = (condition * model_inputs).repeat((2,1))\n",
    "                # if attention_mask is not None: # This is only for decoder_attention_mask (gets when given audio)\n",
    "                #     model_input_attention_mask = (condition * attention_mask).repeat((2,1))\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "            \n",
    "            if model_inputs.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = model_inputs.shape[1] - 1\n",
    "\n",
    "            model_inputs = model_inputs[:, remove_prefix_length:]\n",
    "\n",
    "            # model_inputs = model_inputs[:, -1:]\n",
    "\n",
    "        # Forward Loop\n",
    "        encoder_hidden_states = self.enc_proj(encoder_hidden_states)\n",
    "        encoder_hidden_states = encoder_hidden_states * attention_mask[..., None]\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            input_ids = model_inputs,\n",
    "            attention_mask = None,\n",
    "            encoder_hidden_states = encoder_hidden_states,\n",
    "            encoder_attention_mask = attention_mask,\n",
    "            inputs_embeds = None,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            use_cache = True,\n",
    "            past_key_values = past_key_values,\n",
    "            return_dict = True,\n",
    "            labels = None,\n",
    "            head_mask = None,\n",
    "        )\n",
    "\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :].clone()\n",
    "\n",
    "        # CFG processing if cfg is large enough, aka logits_processlist\n",
    "        if condition:\n",
    "            next_token_scores = self.logits_process(next_token_logits, cfg)\n",
    "        else:\n",
    "            next_token_scores = next_token_logits\n",
    "\n",
    "        next_token_scores = self.logits_warp(next_token_scores, temperature, topk, topp)\n",
    "\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        # next_tokens = torch.argmax(next_token_scores, dim=-1) # Could be added for no sample generation\n",
    "\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "        past_key_values_a = torch.zeros((len(past_key_values), len(past_key_values[0])//2) + tuple(past_key_values[0][0].size()), dtype=torch.float32)\n",
    "        past_key_values_b = torch.zeros((len(past_key_values), len(past_key_values[0])//2) + tuple(past_key_values[0][2].size()), dtype=torch.float32)\n",
    "        for xi, x in enumerate(past_key_values):\n",
    "            for yi, y in enumerate(x):\n",
    "                if y.size(2) == 18:\n",
    "                    past_key_values_b[xi, yi-2] = y\n",
    "                else:\n",
    "                    past_key_values_a[xi, yi] = y\n",
    "\n",
    "        return decoder_input_ids, past_key_values_a, past_key_values_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_torch_script:\n",
    "    sample = Sample(model.decoder, model.enc_to_dec_proj)\n",
    "    sample.eval()\n",
    "    scripted_model = torch.jit.script(sample)  # or torch.jit.trace for simple models\n",
    "    scripted_model.save(f\"{folder}/sample.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_onnx:\n",
    "    sample = Sample(model.decoder, model.enc_to_dec_proj)\n",
    "    sample.eval()\n",
    "\n",
    "    # Define the dynamic axes for variable-length input shapes\n",
    "    dynamic_axes = {\n",
    "        'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'encoder_hidden_states': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'delay_pattern_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'decoder_input_ids': {0: 'batch_size', 1: 'sequence_length'}\n",
    "    }\n",
    "\n",
    "    # Example input shapes (with batch size = 2, sequence length = 10)\n",
    "    dummy_decoder_input_ids = torch.randint(0, 100, (8, 1), dtype=torch.int64)\n",
    "    dummy_attention_mask = torch.randint(0, 100, (2, 18), dtype=torch.int64)\n",
    "    dummy_encoder_hidden_states = torch.randn((2, 18, 768), dtype=torch.float32)\n",
    "    dummy_delay_pattern_mask = torch.randint(0, 100, (8, 260), dtype=torch.int64)\n",
    "    dummy_cfg = torch.tensor(3, dtype=torch.int64)\n",
    "    dummy_temperature = torch.tensor(0.7, dtype=torch.float32)\n",
    "    dummy_topk = torch.tensor(500, dtype=torch.int64)\n",
    "    dummy_topp = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "    # Export the model to ONNX format\n",
    "    torch.onnx.export(\n",
    "        sample,\n",
    "        (\n",
    "            dummy_decoder_input_ids, \n",
    "            dummy_attention_mask, \n",
    "            dummy_encoder_hidden_states, \n",
    "            dummy_delay_pattern_mask, \n",
    "            dummy_cfg, \n",
    "            dummy_temperature, \n",
    "            dummy_topk, \n",
    "            dummy_topp\n",
    "        ),\n",
    "        f\"{folder}/sampler.onnx\",\n",
    "        input_names=[\n",
    "            'decoder_input_ids', \n",
    "            'attention_mask', \n",
    "            'encoder_hidden_states', \n",
    "            'delay_pattern_mask', \n",
    "            'cfg', \n",
    "            'temperature', \n",
    "            'topk', \n",
    "            'topp'\n",
    "        ],\n",
    "        output_names=['decoder_input_ids'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=17\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test torch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_torch_script:\n",
    "    text_encoder_wrapper = TextEncoderWrapper(model.text_encoder)\n",
    "    text_encoder_wrapper.eval()\n",
    "\n",
    "    pre_loop = PreLoop(model.config.decoder.num_codebooks, model.config.decoder.audio_channels)\n",
    "    pre_loop.eval()\n",
    "\n",
    "    sample = Sample(model.decoder, model.enc_to_dec_proj)\n",
    "    sample.eval()\n",
    "\n",
    "    audio_decoder_wrapper = DecodeAudioWrapper(model.audio_encoder)\n",
    "    audio_decoder_wrapper.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_a = torch.zeros((len(past_key_values), len(past_key_values[0])//2) + tuple(past_key_values[0][0].size()), dtype=torch.float32)\n",
    "zeros_b = torch.zeros((len(past_key_values), len(past_key_values[0])//2) + tuple(past_key_values[0][2].size()), dtype=torch.float32)\n",
    "for xi, x in enumerate(past_key_values):\n",
    "    for yi, y in enumerate(x):\n",
    "        if y.size(2) == 18:\n",
    "            zeros_b[xi, yi-2] = y\n",
    "        else:\n",
    "            zeros_a[xi, yi] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: combine the tuples again in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 2, 2, 16, 1, 64])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 2, 2, 16, 18, 64])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 8, -1]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 22\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m         decoder_input_ids, past_key_values \u001b[38;5;241m=\u001b[39m sample(\n\u001b[1;32m     11\u001b[0m             decoder_input_ids, \n\u001b[1;32m     12\u001b[0m             attention_mask, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m             past_key_values\n\u001b[1;32m     20\u001b[0m         )\n\u001b[0;32m---> 22\u001b[0m     output_values \u001b[38;5;241m=\u001b[39m \u001b[43maudio_decoder_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_delay_pattern_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     26\u001b[0m sampling_rate \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maudio_encoder\u001b[38;5;241m.\u001b[39msampling_rate\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mDecodeAudioWrapper.forward\u001b[0;34m(self, output_ids, decoder_delay_pattern_mask, pad_token_id)\u001b[0m\n\u001b[1;32m     18\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_delay_pattern_mask(output_ids, decoder_delay_pattern_mask)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# revert the pattern delay mask by filtering the pad token id\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43moutput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# append the frame dimension back to the audio codes\u001b[39;00m\n\u001b[1;32m     26\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output_ids[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 8, -1]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "if test_torch_script:\n",
    "    with torch.no_grad():\n",
    "        encoded, attention_mask = text_encoder_wrapper(inputs['input_ids'], inputs['attention_mask'], torch.tensor([5], dtype=torch.int64))\n",
    "\n",
    "        decoder_input_ids, decoder_delay_pattern_mask = pre_loop(torch.tensor(1, dtype=torch.int64), None, None, torch.tensor(256, dtype=torch.int64))\n",
    "        past_key_values_a = torch.tensor([-1])\n",
    "        past_key_values_b = torch.tensor([-1])\n",
    "\n",
    "        # for i in range(256-1):\n",
    "        for i in range(1):\n",
    "            decoder_input_ids, past_key_values = sample(\n",
    "                decoder_input_ids, \n",
    "                attention_mask, \n",
    "                encoded, \n",
    "                decoder_delay_pattern_mask, \n",
    "                torch.tensor([5], dtype=torch.int64), \n",
    "                torch.tensor([0.7], dtype=torch.float32), \n",
    "                torch.tensor([500], dtype=torch.int64), \n",
    "                torch.tensor([0.0], dtype=torch.float32),\n",
    "                past_key_values_a,\n",
    "                past_key_values_b\n",
    "            )\n",
    "\n",
    "        output_values = audio_decoder_wrapper(decoder_input_ids, decoder_delay_pattern_mask, torch.tensor([2048], dtype=torch.int64))\n",
    "\n",
    "    from IPython.display import Audio\n",
    "\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    Audio(output_values[0].detach().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Onnx Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 46\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     ort_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids.1\u001b[39m\u001b[38;5;124m'\u001b[39m: decoder_input_ids, \n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopp\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([top_p], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     44\u001b[0m     }\n\u001b[0;32m---> 46\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/audio_token_decoder.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m ort_session\u001b[38;5;241m.\u001b[39mgraph_optimization_level \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mGraphOptimizationLevel\u001b[38;5;241m.\u001b[39mORT_ENABLE_ALL\n",
      "File \u001b[0;32m~/anaconda3/envs/musiclm/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if test_onnx:\n",
    "    ort_session = ort.InferenceSession(f\"{folder}/text_encoder.onnx\")\n",
    "    ort_session.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    input_ids_np = inputs['input_ids'].detach().numpy()\n",
    "    attention_mask_np = inputs['attention_mask'].detach().numpy()\n",
    "\n",
    "    # Run the model\n",
    "    ort_inputs = {\n",
    "        'input_ids': input_ids_np,\n",
    "        'attention_mask': attention_mask_np,\n",
    "        'cfg': np.array([cfg], dtype=np.int64)\n",
    "    }\n",
    "    encoded, attention_mask = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    ort_session = ort.InferenceSession(f\"{folder}/pre_loop.onnx\")\n",
    "    ort_session.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    batch_size = torch.tensor(1, dtype=torch.int64).detach().numpy()\n",
    "    max_length = torch.tensor(256, dtype=torch.int64).detach().numpy()\n",
    "\n",
    "    # Run the model\n",
    "    ort_inputs = {\n",
    "        'batch_size': batch_size,\n",
    "        'max_length': max_length\n",
    "    }\n",
    "    decoder_input_ids, decoder_delay_pattern_mask = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    ort_session = ort.InferenceSession(f\"{folder}/sampler.onnx\")\n",
    "    ort_session.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    for i in range(255):\n",
    "        print(i, end='\\r')\n",
    "        # Run the model\n",
    "        ort_inputs = {\n",
    "            'decoder_input_ids.1': decoder_input_ids, \n",
    "            'attention_mask': attention_mask.astype(np.int64), \n",
    "            'encoder_hidden_states': encoded, \n",
    "            'delay_pattern_mask': decoder_delay_pattern_mask, \n",
    "            'cfg': np.array([cfg], dtype=np.int64), \n",
    "            'temperature': np.array([temperature], dtype=np.float32), \n",
    "            'topk': np.array([top_k], dtype=np.int64), \n",
    "            'topp': np.array([top_p], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "        decoder_input_ids = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "    ort_session = ort.InferenceSession(f\"{folder}/audio_token_decoder.onnx\")\n",
    "    ort_session.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    # Run the model\n",
    "    ort_inputs = {\n",
    "        'output_ids': decoder_input_ids, # We either need to remove the first tokens or add tokens to the decoder delay_pattern mask, check og for inspo\n",
    "        'decoder_delay_pattern_mask': decoder_delay_pattern_mask,\n",
    "        'pad_token_id': np.array([2048], dtype=np.int64)\n",
    "    }\n",
    "\n",
    "    output_values = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "    from IPython.display import Audio\n",
    "\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    Audio(output_values[0], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "\n",
    "# sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "# scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=output_values[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and export the full model\n",
    "\n",
    "Doesnt work bc of problems in attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenWrapper(nn.Module):\n",
    "    def __init__(self, model: MusicgenForConditionalGeneration):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, guidance_scale=3, max_new_tokens=256, temperature=2.0, top_k=500, top_p=0.0):\n",
    "        '''Taken from last section of the model'''\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        output_values = model.generate(**inputs, guidance_scale=guidance_scale.item(), max_new_tokens=max_new_tokens.item(), temperature=temperature.item(), top_k=top_k.item(), top_p=top_p.item())\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musicgen_wrapper = MusicGenWrapper(model)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'}, # Allow variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_guidance_scale = torch.tensor(3, dtype=torch.int64)\n",
    "dummy_max_new_tokens = torch.tensor(256, dtype=torch.int64)\n",
    "dummy_temperature = torch.tensor(2.0, dtype=torch.float32)\n",
    "dummy_top_k = torch.tensor(500, dtype=torch.int64)\n",
    "dummy_top_p = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    musicgen_wrapper,\n",
    "    (inputs['input_ids'], inputs['attention_mask'], dummy_guidance_scale, dummy_max_new_tokens, dummy_temperature, dummy_top_k, dummy_top_p),\n",
    "    f\"{folder}/musicgen.onnx\", \n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'attention_mask',\n",
    "        'guidance_scale',\n",
    "        'max_new_tokens',\n",
    "        'temperature',\n",
    "        'top_k',\n",
    "        'top_p'\n",
    "    ],\n",
    "    output_names=['output_values'],\n",
    "    dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the projection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.past_key_values = None\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states, encoder_attention_mask):\n",
    "        outputs = self.decoder(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = None,\n",
    "            encoder_hidden_states = encoder_hidden_states,\n",
    "            encoder_attention_mask = encoder_attention_mask,\n",
    "            head_mask = None,\n",
    "            cross_attn_head_mask = None,\n",
    "            past_key_values = self.past_key_values,\n",
    "            inputs_embeds = None,\n",
    "            labels = None,\n",
    "            use_cache = True,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            return_dict = True\n",
    "        )\n",
    "\n",
    "        self.past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_wrapper = DecoderWrapper(model.decoder)\n",
    "\n",
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size'},\n",
    "    'encoder_hidden_states': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'encoder_attention_mask': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_input_ids = torch.randint(0, 100, (16, 1), dtype=torch.int64)\n",
    "dummy_encoder_hidden_states = torch.randn((2, 18, 1024), dtype=torch.float32)\n",
    "dummy_encoder_attention_mask = torch.randint(0, 100, (2, 18), dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    decoder_wrapper,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_encoder_hidden_states,\n",
    "        dummy_encoder_attention_mask,\n",
    "    ),\n",
    "    f\"{folder}/decoder.onnx\",\n",
    "    input_names=[\n",
    "        'input_ids',\n",
    "        'encoder_hidden_states',\n",
    "        'encoder_attention_mask',\n",
    "    ],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/audio_token_decoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = output_ids.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = decoder_delay_pattern_mask.detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "pad_token_np = torch.tensor(2048, dtype=torch.int64).detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    # 'input_ids': np.expand_dims(np.concatenate((input_ids_np, attention_mask_np), axis=0), 0),\n",
    "    'output_ids': input_ids_np,\n",
    "    'decoder_delay_pattern_mask': attention_mask_np,\n",
    "    'pad_token_id': pad_token_np\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = f\"{folder}/sampler.onnx\"  # Update this with your ONNX model path\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Print model input names and their shapes\n",
    "print(\"Model Inputs:\")\n",
    "for input_tensor in onnx_model.graph.input:\n",
    "    print(f\"Input name: {input_tensor.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.text_encoder.encoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.quantizer.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder._decode_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forward_method_code.py\", \"w\") as file:\n",
    "    file.write(inspect.getsource(model.audio_encoder.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic axes for variable-length input shapes\n",
    "dynamic_axes = {\n",
    "    'encoder_hidden_states_in': {0: 'batch_size', 1: 'sequence_length'},  # Allow variable batch size and sequence length\n",
    "    'encoder_hidden_states_out': {0: 'batch_size', 1: 'sequence_length'}  # Output will also have variable batch size and sequence length\n",
    "}\n",
    "\n",
    "# Example input shapes (with batch size = 2, sequence length = 10)\n",
    "dummy_encoder_hidden_states = torch.randint(0, 100, (2, 12, 768), dtype=torch.float32)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model.enc_to_dec_proj,                             # Model to export\n",
    "    (dummy_encoder_hidden_states,),                             # Example input tuple\n",
    "    f\"{folder}/enc_to_dec_proj.onnx\",               # Export path\n",
    "    input_names=['encoder_hidden_states_in'],          # Input tensor names\n",
    "    output_names=['encoder_hidden_states_out'],         # Output tensor name\n",
    "    dynamic_axes=dynamic_axes                       # Dynamic axes for variable-length inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the audio_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(f\"{folder}/text_encoder.onnx\")\n",
    "\n",
    "# Prepare input data (assuming you already have input_ids and attention_mask as PyTorch tensors)\n",
    "input_ids_np = inputs['input_ids'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "attention_mask_np = inputs['attention_mask'].detach().numpy()  # Convert to NumPy arrays if they're in PyTorch tensors\n",
    "\n",
    "# Run the model\n",
    "ort_inputs = {\n",
    "    'input_ids': input_ids_np,\n",
    "    'attention_mask': attention_mask_np,\n",
    "    'cfg': np.array([3], dtype=np.int64)\n",
    "}\n",
    "encoded = ort_session.run(None, ort_inputs)[0]\n",
    "encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
